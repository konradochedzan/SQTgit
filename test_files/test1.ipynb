{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634ffe2e",
   "metadata": {},
   "source": [
    "# READ ME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af8c6a",
   "metadata": {},
   "source": [
    "This is the training pipeline that was used as of day 19.06.2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3890e176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:15:03.439600Z",
     "start_time": "2025-06-18T20:15:03.379956Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import warnings\n",
    "import random\n",
    "from Base_models import FeedForwardPredictor, AutoEncoder, ElasticNetLoss\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # might slow down but ensures reproducibility\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed()\n",
    "\n",
    "def calculate_sharpe_ratio(returns: np.ndarray, rf_rate: float = 0.02) -> float:\n",
    "    \"\"\"Calculate annualized Sharpe ratio\"\"\"\n",
    "    if len(returns) == 0 or np.std(returns) == 0:\n",
    "        return 0.0\n",
    "    excess_returns = returns - rf_rate/12  # Monthly risk-free rate\n",
    "    return np.sqrt(12) * np.mean(excess_returns) / np.std(excess_returns)\n",
    "\n",
    "def calculate_sortino_ratio(returns: np.ndarray, rf_rate: float = 0.02) -> float:\n",
    "    \"\"\"Annualised Sortino ratio (down-side risk only).\"\"\"\n",
    "    if len(returns) == 0:\n",
    "        return 0.0\n",
    "    excess = returns - rf_rate / 12          # monthly risk-free\n",
    "    downside = excess[excess < 0]\n",
    "    if downside.size == 0:\n",
    "        return np.inf\n",
    "    return np.sqrt(12) * excess.mean() / downside.std()\n",
    "\n",
    "def hit_rate(pred: np.ndarray, targ: np.ndarray) -> float:\n",
    "    \"\"\"Percentage of months the sign is predicted correctly.\"\"\"\n",
    "    return np.mean(np.sign(pred) == np.sign(targ))\n",
    "\n",
    "def prepare_data_for_model(X: np.ndarray, y: np.ndarray, model_type: str, \n",
    "                          seq_length: int = 12) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Prepare data based on model type (feedforward vs LSTM)\"\"\"\n",
    "    if model_type.lower() in ['lstm', 'transformer']:\n",
    "        # For LSTM, create sequences\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(seq_length, len(X)):\n",
    "            X_seq.append(X[i-seq_length:i])\n",
    "            y_seq.append(y[i])\n",
    "        return torch.FloatTensor(np.array(X_seq)), torch.FloatTensor(np.array(y_seq))\n",
    "    else:\n",
    "        # For feedforward, use as-is\n",
    "        return torch.FloatTensor(X), torch.FloatTensor(y)\n",
    "\n",
    "def train_autoencoder(X_train: np.ndarray, encoding_dim: int, epochs: int = 100, \n",
    "                     lr: float = 0.001, device: str = 'cpu') -> AutoEncoder:\n",
    "    \"\"\"Train autoencoder on training data only\"\"\"\n",
    "    input_dim = X_train.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    dataset = TensorDataset(X_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    autoencoder.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            x_batch = batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(x_batch)\n",
    "            loss = criterion(reconstructed, x_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    autoencoder.eval()\n",
    "    return autoencoder\n",
    "\n",
    "def train_model_epoch(model: nn.Module, dataloader: DataLoader, criterion: nn.Module,\n",
    "                     optimizer: optim.Optimizer, scheduler=None, model_type: str = 'feedforward', device: str = 'cpu') -> float:\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch).squeeze()\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        if model_type.lower() == 'transformer':\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler and isinstance(scheduler, optim.lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model: nn.Module, dataloader: DataLoader, criterion: nn.Module,\n",
    "                  device: str) -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Evaluate model and return loss, predictions, and targets\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch).squeeze()\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    return total_loss / len(dataloader), np.array(all_predictions), np.array(all_targets)\n",
    "\n",
    "def get_optimizer_for_model(model: nn.Module, model_type: str, lr: float = 0.001):\n",
    "    \"\"\"Get appropriate optimizer based on model type.\"\"\"\n",
    "    if model_type.lower() == 'transformer':\n",
    "        # AdamW often works better for transformers\n",
    "        return optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    else:\n",
    "        # Standard Adam for other models\n",
    "        return optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def get_learning_rate_scheduler(optimizer, model_type: str, num_training_steps: int = None):\n",
    "    \"\"\"Get appropriate learning rate scheduler based on model type.\"\"\"\n",
    "    if model_type.lower() == 'transformer' and num_training_steps:\n",
    "        # Warmup scheduler for transformers\n",
    "        return optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, \n",
    "            max_lr=optimizer.param_groups[0]['lr'] * 10,\n",
    "            total_steps=num_training_steps,\n",
    "            pct_start=0.1  # 10% warmup\n",
    "        )\n",
    "    else:\n",
    "        # Simple step scheduler for other models\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "\n",
    "def sp500_training_pipeline(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    dates: pd.DatetimeIndex,\n",
    "    model_class: type,\n",
    "    model_kwargs: Dict[str, Any],\n",
    "    model_type: str = 'feedforward',  # 'feedforward' or 'lstm'\n",
    "    window_strategy: str = 'rolling',  # 'rolling' or 'expanding'\n",
    "    train_window_years: int = 3,\n",
    "    test_window_years: int = 1,\n",
    "    use_autoencoder: bool = True,\n",
    "    encoding_dim: int = 10,\n",
    "    walk_forward_cv: bool = True,\n",
    "    cv_months: int = 2,\n",
    "    seq_length: int = 12,  # For LSTM models\n",
    "    epochs: int = 100,\n",
    "    lr: float = 0.001,\n",
    "    batch_size: int = 32,\n",
    "    alpha = 0.1,  # ElasticNet alpha\n",
    "    l1_ratio = 0.5,  # ElasticNet l1_ratio\n",
    "    device: str = 'cpu',\n",
    "    random_seed: int = 42,\n",
    "    plot_results: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive S&P 500 returns prediction pipeline with rolling/expanding windows.\n",
    "    Fixed for daily data input.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Convert dates to pandas DatetimeIndex if not already\n",
    "    if not isinstance(dates, pd.DatetimeIndex):\n",
    "        dates = pd.DatetimeIndex(dates)\n",
    "    \n",
    "    # Initialize storage\n",
    "    results = {\n",
    "        'fold_results': [],\n",
    "        'scalers': [],\n",
    "        'autoencoders': [],\n",
    "        'models': [],\n",
    "        'all_train_predictions': [],\n",
    "        'all_train_targets': [],\n",
    "        'all_test_predictions': [],\n",
    "        'all_test_targets': [],\n",
    "        'all_train_dates': [],\n",
    "        'all_test_dates': [],\n",
    "        'metrics': {\n",
    "            'train_mse': [],\n",
    "            'test_mse': [],\n",
    "            'train_sharpe': [],\n",
    "            'test_sharpe': [],\n",
    "            'train_mae': [], 'test_mae': [],\n",
    "            'train_r2':  [], 'test_r2':  [],\n",
    "            'train_sortino': [], 'test_sortino': [],\n",
    "            'train_hit': [], 'test_hit': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create date-based windows\n",
    "    start_date = dates[0]\n",
    "    end_date = dates[-1]\n",
    "    \n",
    "    # Calculate fold start dates\n",
    "    fold_start_dates = []\n",
    "    current_date = start_date + pd.DateOffset(years=train_window_years)\n",
    "    \n",
    "    while current_date + pd.DateOffset(years=test_window_years) <= end_date:\n",
    "        fold_start_dates.append(current_date)\n",
    "        current_date += pd.DateOffset(years=test_window_years)\n",
    "    \n",
    "    n_folds = len(fold_start_dates)\n",
    "    \n",
    "    print(f\"Starting {window_strategy} window training with {n_folds} folds\")\n",
    "    print(f\"Train window: {train_window_years} years, Test window: {test_window_years} years\")\n",
    "    print(f\"Data period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    fold_predictions = []\n",
    "    fold_targets = []\n",
    "    fold_dates = []\n",
    "    \n",
    "    for fold_idx, test_start_date in enumerate(fold_start_dates):\n",
    "        print(f\"\\n--- Fold {fold_idx + 1}/{n_folds} ---\")\n",
    "        \n",
    "        # Define test period\n",
    "        test_end_date = test_start_date + pd.DateOffset(years=test_window_years)\n",
    "        \n",
    "        # Define train period based on strategy\n",
    "        if window_strategy == 'rolling':\n",
    "            train_start_date = test_start_date - pd.DateOffset(years=train_window_years)\n",
    "        else:  # expanding\n",
    "            train_start_date = start_date\n",
    "        \n",
    "        train_end_date = test_start_date\n",
    "        \n",
    "        # Get indices for train and test periods\n",
    "        train_mask = (dates >= train_start_date) & (dates < train_end_date)\n",
    "        test_mask = (dates >= test_start_date) & (dates < test_end_date)\n",
    "        \n",
    "        # Check if we have enough data\n",
    "        if train_mask.sum() == 0 or test_mask.sum() == 0:\n",
    "            print(f\"Skipping fold {fold_idx + 1}: insufficient data\")\n",
    "            continue\n",
    "        \n",
    "        # Extract data\n",
    "        train_indices = np.where(train_mask)[0]\n",
    "        test_indices = np.where(test_mask)[0]\n",
    "        X_train = X[train_mask]\n",
    "        X_test  = X[test_mask]\n",
    "        y_train = y[train_mask]\n",
    "        y_test  = y[test_mask]\n",
    "        \n",
    "        train_dates = dates[train_indices]\n",
    "        test_dates = dates[test_indices]\n",
    "        \n",
    "        print(f\"Train period: {train_dates[0].strftime('%Y-%m-%d')} to {train_dates[-1].strftime('%Y-%m-%d')} ({len(train_dates)} days)\")\n",
    "        print(f\"Test period: {test_dates[0].strftime('%Y-%m-%d')} to {test_dates[-1].strftime('%Y-%m-%d')} ({len(test_dates)} days)\")\n",
    "        \n",
    "        # Feature standardization (fit only on training data)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Autoencoder training and encoding (if enabled)\n",
    "        autoencoder = None\n",
    "        if use_autoencoder:\n",
    "            print(f\"Training autoencoder with encoding dimension {encoding_dim}\")\n",
    "            autoencoder = train_autoencoder(X_train_scaled, encoding_dim, \n",
    "                                          epochs=50, lr=lr, device=device)\n",
    "            \n",
    "            # Encode features\n",
    "            with torch.no_grad():\n",
    "                X_train_encoded = autoencoder.encode(torch.FloatTensor(X_train_scaled).to(device)).cpu().numpy()\n",
    "                X_test_encoded = autoencoder.encode(torch.FloatTensor(X_test_scaled).to(device)).cpu().numpy()\n",
    "        else:\n",
    "            X_train_encoded = X_train_scaled\n",
    "            X_test_encoded = X_test_scaled\n",
    "        \n",
    "        # Prepare data for model type\n",
    "        X_train_tensor, y_train_tensor = prepare_data_for_model(X_train_encoded, y_train, \n",
    "                                                              model_type, seq_length)\n",
    "        X_test_tensor, y_test_tensor = prepare_data_for_model(X_test_encoded, y_test, \n",
    "                                                            model_type, seq_length)\n",
    "        \n",
    "        # Adjust dates for LSTM (due to sequence creation)\n",
    "        if model_type.lower()in ['lstm', 'transformer']:\n",
    "            train_dates_adj = train_dates[seq_length:]\n",
    "            test_dates_adj = test_dates[seq_length:]\n",
    "        else:\n",
    "            train_dates_adj = train_dates\n",
    "            test_dates_adj = test_dates\n",
    "        \n",
    "        # Walk-forward cross-validation (optional) - modified for daily data\n",
    "        best_model_state = None\n",
    "        if walk_forward_cv and len(X_train_tensor) > cv_months * 20:  # Approximate days per month\n",
    "            print(f\"Performing walk-forward CV with {cv_months} months validation\")\n",
    "            \n",
    "            # Split training data for CV - use approximate days\n",
    "            cv_days = cv_months * 20  # Rough approximation\n",
    "            cv_split = len(X_train_tensor) - cv_days\n",
    "            X_train_cv = X_train_tensor[:cv_split]\n",
    "            y_train_cv = y_train_tensor[:cv_split]\n",
    "            X_val_cv = X_train_tensor[cv_split:]\n",
    "            y_val_cv = y_train_tensor[cv_split:]\n",
    "            \n",
    "            # Initialize model for CV\n",
    "            input_dim = X_train_tensor.shape[-1]\n",
    "            model = model_class(input_dim=input_dim, **model_kwargs).to(device)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = get_optimizer_for_model(model, model_type, lr)\n",
    "            num_training_steps = (len(X_train_cv) // batch_size) * epochs\n",
    "\n",
    "            scheduler = get_learning_rate_scheduler(optimizer, model_type, num_training_steps=num_training_steps)\n",
    "            \n",
    "            \n",
    "            # Create data loaders for CV\n",
    "            train_cv_dataset = TensorDataset(X_train_cv, y_train_cv)\n",
    "            train_cv_loader = DataLoader(train_cv_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_cv_dataset = TensorDataset(X_val_cv, y_val_cv)\n",
    "            val_cv_loader = DataLoader(val_cv_dataset, batch_size=batch_size, shuffle=False)\n",
    "            \n",
    "            best_val_loss = float('inf')\n",
    "            patience = 10\n",
    "            patience_counter = 0\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                train_loss = train_model_epoch(model, train_cv_loader, criterion, optimizer, device)\n",
    "                val_loss, _, _ = evaluate_model(model, val_cv_loader, criterion, device)\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    best_model_state = model.state_dict().copy()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                        break\n",
    "        \n",
    "        # Final model training on full training set\n",
    "        input_dim = X_train_tensor.shape[-1]\n",
    "        model = model_class(input_dim=input_dim, **model_kwargs).to(device)\n",
    "        \n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "        \n",
    "        criterion = ElasticNetLoss(model=model,alpha = alpha, l1_ratio = l1_ratio)\n",
    "        optimizer = get_optimizer_for_model(model, model_type, lr)\n",
    "        num_training_steps = (len(X_train_tensor) // batch_size) * epochs\n",
    "        scheduler = get_learning_rate_scheduler(optimizer, model_type, num_training_steps=None)\n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Final training\n",
    "        print(\"Training final model...\")\n",
    "        final_epochs = epochs // 2 if walk_forward_cv else epochs\n",
    "        for epoch in range(final_epochs):\n",
    "            train_loss = train_model_epoch(model, train_loader, criterion, optimizer, scheduler=scheduler, model_type=model_type,device=device)\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{final_epochs}, Loss: {train_loss:.6f}\")\n",
    "        \n",
    "        # Evaluate on both train and test sets\n",
    "        train_loss, train_predictions, train_targets = evaluate_model(model, train_loader, criterion, device)\n",
    "        test_loss, test_predictions, test_targets = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_mse = mean_squared_error(train_targets, train_predictions)\n",
    "        test_mse = mean_squared_error(test_targets, test_predictions)\n",
    "\n",
    "        train_mae   = mean_absolute_error(train_targets, train_predictions)\n",
    "        test_mae    = mean_absolute_error(test_targets,  test_predictions)\n",
    "\n",
    "        train_r2    = r2_score(train_targets, train_predictions)\n",
    "        test_r2     = r2_score(test_targets,  test_predictions)\n",
    "\n",
    "        train_sharpe = calculate_sharpe_ratio(train_predictions)\n",
    "        test_sharpe = calculate_sharpe_ratio(test_predictions)\n",
    "\n",
    "        train_sortino = calculate_sortino_ratio(train_predictions)\n",
    "        test_sortino  = calculate_sortino_ratio(test_predictions)\n",
    "\n",
    "        train_hit     = hit_rate(train_predictions, train_targets)\n",
    "        test_hit      = hit_rate(test_predictions,  test_targets)\n",
    "        \n",
    "        print(f\"Train MSE {train_mse:.6f} | MAE {train_mae:.6f} | R² {train_r2:.4f} | Sharpe {train_sharpe:.3f} | Sortino {train_sortino:.3f} | Hit {train_hit:.2%}\")\n",
    "        print(f\"Test  MSE {test_mse:.6f} | MAE {test_mae:.6f} | R² {test_r2:.4f} | Sharpe {test_sharpe:.3f} | Sortino {test_sortino:.3f} | Hit {test_hit:.2%}\")\n",
    "\n",
    "        \n",
    "        # Store results\n",
    "        fold_result = {\n",
    "            'fold': fold_idx,\n",
    "            'train_period': (train_dates[0], train_dates[-1]),\n",
    "            'test_period': (test_dates[0], test_dates[-1]),\n",
    "            'train_mse': train_mse,\n",
    "            'test_mse': test_mse,\n",
    "            'train_sharpe': train_sharpe,\n",
    "            'test_sharpe': test_sharpe,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'train_sortino': train_sortino,\n",
    "            'test_sortino': test_sortino,\n",
    "            'train_hit': train_hit,\n",
    "            'test_hit': test_hit,\n",
    "            'train_predictions': train_predictions,\n",
    "            'test_predictions': test_predictions,\n",
    "            'train_targets': train_targets,\n",
    "            'test_targets': test_targets,\n",
    "            'train_dates': train_dates_adj,\n",
    "            'test_dates': test_dates_adj\n",
    "        }\n",
    "        \n",
    "        results['fold_results'].append(fold_result)\n",
    "        results['scalers'].append(scaler)\n",
    "        results['autoencoders'].append(autoencoder)\n",
    "        results['models'].append(model.state_dict())\n",
    "        \n",
    "        # Accumulate for overall plotting\n",
    "        results['all_train_predictions'].extend(train_predictions)\n",
    "        results['all_train_targets'].extend(train_targets)\n",
    "        results['all_test_predictions'].extend(test_predictions)\n",
    "        results['all_test_targets'].extend(test_targets)\n",
    "        results['all_train_dates'].extend(train_dates_adj)\n",
    "        results['all_test_dates'].extend(test_dates_adj)\n",
    "        \n",
    "        results['metrics']['train_mse'].append(train_mse)\n",
    "        results['metrics']['test_mse'].append(test_mse)\n",
    "        results['metrics']['train_sharpe'].append(train_sharpe)\n",
    "        results['metrics']['test_sharpe'].append(test_sharpe)\n",
    "        results['metrics']['train_mae'].append(train_mae)\n",
    "        results['metrics']['test_mae'].append(test_mae)\n",
    "        results['metrics']['train_r2'].append(train_r2)\n",
    "        results['metrics']['test_r2'].append(test_r2)\n",
    "        results['metrics']['train_sortino'].append(train_sortino)\n",
    "        results['metrics']['test_sortino'].append(test_sortino)\n",
    "        results['metrics']['train_hit'].append(train_hit)\n",
    "        results['metrics']['test_hit'].append(test_hit)\n",
    "        \n",
    "        # Store for individual fold plotting\n",
    "        fold_predictions.append((train_predictions, test_predictions))\n",
    "        fold_targets.append((train_targets, test_targets))\n",
    "        fold_dates.append((train_dates_adj, test_dates_adj))\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    if results['metrics']['train_mse']:\n",
    "        results['overall_metrics'] = {\n",
    "            'avg_train_mse': np.mean(results['metrics']['train_mse']),\n",
    "            'avg_test_mse': np.mean(results['metrics']['test_mse']),\n",
    "            'avg_train_sharpe': np.mean(results['metrics']['train_sharpe']),\n",
    "            'avg_test_sharpe': np.mean(results['metrics']['test_sharpe']),\n",
    "            'std_train_mse': np.std(results['metrics']['train_mse']),\n",
    "            'std_test_mse': np.std(results['metrics']['test_mse']),\n",
    "            'std_train_sharpe': np.std(results['metrics']['train_sharpe']),\n",
    "            'std_test_sharpe': np.std(results['metrics']['test_sharpe']),\n",
    "            'avg_train_mae': np.mean(results['metrics']['train_mae']),\n",
    "            'avg_test_mae': np.mean(results['metrics']['test_mae']),\n",
    "            'std_train_mae': np.std(results['metrics']['train_mae']),\n",
    "            'std_test_mae': np.std(results['metrics']['test_mae']),\n",
    "            'avg_train_r2': np.mean(results['metrics']['train_r2']),\n",
    "            'avg_test_r2': np.mean(results['metrics']['test_r2']),\n",
    "            'std_train_r2': np.std(results['metrics']['train_r2']),\n",
    "            'std_test_r2': np.std(results['metrics']['test_r2']),\n",
    "            'avg_train_sortino': np.mean(results['metrics']['train_sortino']),\n",
    "            'avg_test_sortino': np.mean(results['metrics']['test_sortino']),\n",
    "            'std_train_sortino': np.std(results['metrics']['train_sortino']),\n",
    "            'std_test_sortino': np.std(results['metrics']['test_sortino']),\n",
    "            'avg_train_hit': np.mean(results['metrics']['train_hit']),\n",
    "            'avg_test_hit': np.mean(results['metrics']['test_hit']),\n",
    "            'std_train_hit': np.std(results['metrics']['train_hit']),\n",
    "            'std_test_hit': np.std(results['metrics']['test_hit'])\n",
    "        }\n",
    "        \n",
    "        print(\"\\n=== OVERALL RESULTS ===\")\n",
    "        print(f\"Average Train MSE: {results['overall_metrics']['avg_train_mse']:.6f} ± {results['overall_metrics']['std_train_mse']:.6f}\")\n",
    "        print(f\"Average Test MSE: {results['overall_metrics']['avg_test_mse']:.6f} ± {results['overall_metrics']['std_test_mse']:.6f}\")\n",
    "        print(f\"Average Train Sharpe: {results['overall_metrics']['avg_train_sharpe']:.4f} ± {results['overall_metrics']['std_train_sharpe']:.4f}\")\n",
    "        print(f\"Average Test Sharpe: {results['overall_metrics']['avg_test_sharpe']:.4f} ± {results['overall_metrics']['std_test_sharpe']:.4f}\")\n",
    "        print(f\"Average Train MAE: {results['overall_metrics']['avg_train_mae']:.6f} ± {results['overall_metrics']['std_train_mae']:.6f}\")\n",
    "        print(f\"Average Test MAE: {results['overall_metrics']['avg_test_mae']:.6f} ± {results['overall_metrics']['std_test_mae']:.6f}\")\n",
    "        print(f\"Average Train R²: {results['overall_metrics']['avg_train_r2']:.4f} ± {results['overall_metrics']['std_train_r2']:.4f}\")\n",
    "        print(f\"Average Test R²: {results['overall_metrics']['avg_test_r2']:.4f} ± {results['overall_metrics']['std_test_r2']:.4f}\")\n",
    "        print(f\"Average Train Sortino: {results['overall_metrics']['avg_train_sortino']:.4f} ± {results['overall_metrics']['std_train_sortino']:.4f}\") \n",
    "        print(f\"Average Test Sortino: {results['overall_metrics']['avg_test_sortino']:.4f} ± {results['overall_metrics']['std_test_sortino']:.4f}\")\n",
    "        print(f\"Average Train Hit Rate: {results['overall_metrics']['avg_train_hit']:.2%} ± {results['overall_metrics']['std_train_hit']:.2%}\")\n",
    "        print(f\"Average Test Hit Rate: {results['overall_metrics']['avg_test_hit']:.2%} ± {results['overall_metrics']['std_test_hit']:.2%}\")\n",
    "\n",
    "    \n",
    "    # Plotting\n",
    "    if plot_results and fold_predictions:\n",
    "        n_plots = len(fold_predictions)\n",
    "        fig, axes = plt.subplots(n_plots, 1, figsize=(15, 4 * n_plots))\n",
    "        if n_plots == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, (fold_pred, fold_targ, fold_date) in enumerate(zip(fold_predictions, fold_targets, fold_dates)):\n",
    "            train_pred, test_pred = fold_pred\n",
    "            train_targ, test_targ = fold_targ\n",
    "            train_date, test_date = fold_date\n",
    "            \n",
    "            ax = axes[i]\n",
    "\n",
    "            # Plot actual returns (always black)\n",
    "            ax.plot(train_date, train_targ, 'k-', label='Actual Returns', linewidth=1)\n",
    "            ax.plot(test_date, test_targ, 'k-', linewidth=1)\n",
    "            \n",
    "            # Plot predictions (blue for train, red for test)\n",
    "            ax.plot(train_date, train_pred, 'b-', label='Train Predictions', alpha=0.7)\n",
    "            ax.plot(test_date, test_pred, 'r-', label='Test Predictions', alpha=0.7)\n",
    "            \n",
    "            # Add vertical line to separate train/test\n",
    "            if len(test_date) > 0:\n",
    "                ax.axvline(x=test_date[0], color='gray', linestyle='--', alpha=0.5)\n",
    "            \n",
    "            ax.set_title(f'Fold {i+1}: Predictions vs Actual Returns')\n",
    "            ax.set_ylabel('Returns')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Overall performance plot\n",
    "        if len(results['all_train_dates']) > 0:\n",
    "            plt.figure(figsize=(15, 6))\n",
    "            \n",
    "            # Combine and sort all data by date\n",
    "            all_data = []\n",
    "            for train_pred, train_targ, train_date in zip(\n",
    "                [results['fold_results'][i]['train_predictions'] for i in range(len(results['fold_results']))],\n",
    "                [results['fold_results'][i]['train_targets'] for i in range(len(results['fold_results']))],\n",
    "                [results['fold_results'][i]['train_dates'] for i in range(len(results['fold_results']))]\n",
    "            ):\n",
    "                for pred, targ, date in zip(train_pred, train_targ, train_date):\n",
    "                    all_data.append((date, targ, pred, 'train'))\n",
    "            \n",
    "            for test_pred, test_targ, test_date in zip(\n",
    "                [results['fold_results'][i]['test_predictions'] for i in range(len(results['fold_results']))],\n",
    "                [results['fold_results'][i]['test_targets'] for i in range(len(results['fold_results']))],\n",
    "                [results['fold_results'][i]['test_dates'] for i in range(len(results['fold_results']))]\n",
    "            ):\n",
    "                for pred, targ, date in zip(test_pred, test_targ, test_date):\n",
    "                    all_data.append((date, targ, pred, 'test'))\n",
    "            \n",
    "            # Sort by date\n",
    "            all_data.sort(key=lambda x: x[0])\n",
    "            \n",
    "            # Separate data\n",
    "            dates_all = [x[0] for x in all_data]\n",
    "            targets_all = [x[1] for x in all_data]\n",
    "            predictions_all = [x[2] for x in all_data]\n",
    "            types_all = [x[3] for x in all_data]\n",
    "            \n",
    "            # Plot\n",
    "            plt.plot(dates_all, targets_all, 'k-', label='Actual Returns', linewidth=1)\n",
    "            \n",
    "            # Plot predictions with different colors\n",
    "            train_mask = np.array(types_all) == 'train'\n",
    "            test_mask = np.array(types_all) == 'test'\n",
    "            \n",
    "            if np.any(train_mask):\n",
    "                plt.plot(np.array(dates_all)[train_mask], np.array(predictions_all)[train_mask], \n",
    "                        'b.', label='Train Predictions', alpha=0.7, markersize=3)\n",
    "            if np.any(test_mask):\n",
    "                plt.plot(np.array(dates_all)[test_mask], np.array(predictions_all)[test_mask], \n",
    "                        'r.', label='Test Predictions', alpha=0.7, markersize=3)\n",
    "            \n",
    "            plt.title('Overall Model Performance: Predictions vs Actual Returns')\n",
    "            plt.ylabel('Returns')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7a95400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:15:07.884369Z",
     "start_time": "2025-06-18T20:15:07.847320Z"
    }
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"data_non_std.csv\",parse_dates=[\"Unnamed: 0\"]).rename(columns={'Unnamed: 0': 'Date'}) \n",
    "features = df.drop(columns=[\"returns\", \"Date\"])\n",
    "target = df[\"returns\"].values.astype(np.float32)\n",
    "dates = pd.to_datetime(df[\"Date\"]).astype('datetime64[ns]').tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61acb33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:29:34.991988Z",
     "start_time": "2025-06-18T20:15:10.048397Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleFeedForward(nn.Module):\n",
    "    \"\"\"Example feedforward model\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=200, dropout=0):\n",
    "        super(SimpleFeedForward, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"Example LSTM model\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=200, num_layers=2, dropout=0):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])  # Use last time step output\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 model_dim: int = 128,   # divisible by nhead\n",
    "                 nhead: int = 8,\n",
    "                 num_layers: int = 2,\n",
    "                 dropout: float = 0.1,\n",
    "                 max_seq_length: int = 500):  # for positional encoding\n",
    "        super().__init__()\n",
    "\n",
    "        # project raw features to model_dim\n",
    "        self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "        \n",
    "        # Add positional encoding \n",
    "        self.pos_encoding = PositionalEncoding(model_dim, dropout, max_seq_length)\n",
    "\n",
    "        # vanilla encoder stack\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=model_dim,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=model_dim * 4,  # standard practice\n",
    "                dropout=dropout,\n",
    "                activation='gelu',  # often works better than relu for transformers\n",
    "                batch_first=True        # so x is (B, T, F)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # regression head with additional processing\n",
    "        self.norm = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(model_dim, 1)\n",
    "\n",
    "    def forward(self, x):               # x: (B, T, input_dim)\n",
    "        x = self.input_proj(x)          # (B, T, model_dim)\n",
    "        x = self.pos_encoding(x)        # Add positional information\n",
    "        x = self.encoder(x)             # (B, T, model_dim)\n",
    "        x = self.norm(x[:, -1])         # Layer norm on last time step\n",
    "        x = self.dropout(x)             # Additional dropout\n",
    "        return self.fc(x)               # (B, 1)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.transpose(0, 1)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "results_ff = sp500_training_pipeline(\n",
    "        X=features,\n",
    "        y=target,\n",
    "        dates=dates,\n",
    "        model_class=SimpleLSTM,\n",
    "        model_kwargs={\n",
    "            'hidden_dim': 10,\n",
    "            'num_layers': 2,\n",
    "            'dropout': 0.1\n",
    "        },\n",
    "        model_type='lstm',\n",
    "        window_strategy='rolling',\n",
    "        train_window_years=3,\n",
    "        test_window_years=1,\n",
    "        use_autoencoder=True,\n",
    "        encoding_dim=10,\n",
    "        walk_forward_cv=False,\n",
    "        cv_months=12,\n",
    "        epochs=80,\n",
    "        alpha=0,\n",
    "        l1_ratio=0,\n",
    "        plot_results=True\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (volatilityenv)",
   "language": "python",
   "name": "volatilityenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
