{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0614ffed",
   "metadata": {},
   "source": [
    "# READ ME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea50809",
   "metadata": {},
   "source": [
    "This is a training pipeline as of 20.06.2025 where we start to include more complex predictive models in our project. Testing Temporal Fusion Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15d8467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T12:44:05.218185Z",
     "start_time": "2025-06-20T12:44:05.211911Z"
    }
   },
   "outputs": [],
   "source": [
    "# import torch, sympy\n",
    "# print(\"torch :\", torch.__version__)\n",
    "# print(\"sympy :\", sympy.__version__)\n",
    "# print(\"sympy has printing ?\", hasattr(sympy, \"printing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbb4d9b999249ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T15:29:59.202951Z",
     "start_time": "2025-06-20T15:29:44.092374Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import warnings\n",
    "import random\n",
    "from Base_models import AutoEncoder, ElasticNetLoss\n",
    "import importlib\n",
    "import TM_models\n",
    "importlib.reload(TM_models)\n",
    "from TM_models import TemporalFusionTransformer, TemporalFusionTransformer2\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # might slow down but ensures reproducibility\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "def calculate_sharpe_ratio(strategy_returns: np.ndarray, rf_rate: float) -> float:\n",
    "    \"\"\"Calculate annualized Sharpe ratio using strategy returns and actual risk-free rate\"\"\"\n",
    "    if len(strategy_returns) == 0 or np.std(strategy_returns) == 0:\n",
    "        return 0.0\n",
    "    # rf_rate is already annual, convert to daily\n",
    "    daily_rf = rf_rate / 252  # Assuming daily data\n",
    "    excess_returns = strategy_returns - daily_rf\n",
    "    return np.sqrt(252) * np.mean(excess_returns) / np.std(strategy_returns)\n",
    "\n",
    "def calculate_sortino_ratio(strategy_returns: np.ndarray, rf_rate: float) -> float:\n",
    "    \"\"\"Calculate annualized Sortino ratio using strategy returns and actual risk-free rate\"\"\"\n",
    "    if len(strategy_returns) == 0:\n",
    "        return 0.0\n",
    "    # rf_rate is already annual, convert to daily\n",
    "    daily_rf = rf_rate / 252  # Assuming daily data\n",
    "    excess_returns = strategy_returns - daily_rf\n",
    "    downside_returns = excess_returns[excess_returns < 0]\n",
    "    if len(downside_returns) == 0:\n",
    "        return np.inf\n",
    "    return np.sqrt(252) * np.mean(excess_returns) / np.std(downside_returns)\n",
    "\n",
    "def hit_rate(pred: np.ndarray, targ: np.ndarray) -> float:\n",
    "    \"\"\"Percentage of months the sign is predicted correctly.\"\"\"\n",
    "    return np.mean(np.sign(pred) == np.sign(targ))\n",
    "\n",
    "def train_autoencoder(X_train: np.ndarray, encoding_dim: int, epochs: int = 100,\n",
    "                     lr: float = 0.001, device: str = 'cpu') -> AutoEncoder:\n",
    "    \"\"\"Train autoencoder on training data only\"\"\"\n",
    "    input_dim = X_train.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "\n",
    "    X_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    dataset = TensorDataset(X_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    autoencoder.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            x_batch = batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(x_batch)\n",
    "            loss = criterion(reconstructed, x_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    autoencoder.eval()\n",
    "    return autoencoder\n",
    "\n",
    "def train_model_epoch(model: nn.Module, dataloader: DataLoader, criterion: nn.Module,\n",
    "                     optimizer: optim.Optimizer, scheduler=None, model_type: str = 'feedforward', device: str = 'cpu') -> float:\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch).squeeze()\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        if model_type.lower() == 'transformer':\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler and isinstance(scheduler, optim.lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# def evaluate_model(model: nn.Module, dataloader: DataLoader, criterion: nn.Module,\n",
    "#                   device: str) -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "#     \"\"\"Evaluate model and return loss, predictions, and targets\"\"\"\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     all_predictions = []\n",
    "#     all_targets = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for X_batch, y_batch in dataloader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             predictions = model(X_batch).squeeze()\n",
    "#             loss = criterion(predictions, y_batch)\n",
    "#             total_loss += loss.item()\n",
    "#             all_predictions.extend(predictions.cpu().numpy())\n",
    "#             all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "#     return total_loss / len(dataloader), np.array(all_predictions), np.array(all_targets)\n",
    "\n",
    "def evaluate_model(model: nn.Module,\n",
    "                   dataloader: DataLoader,\n",
    "                   criterion: nn.Module,\n",
    "                   device: str) -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # ── 1. keep the batch axis ───────────────────────────────\n",
    "            predictions = model(X_batch).squeeze(-1)      # shape: (batch_size,)\n",
    "            y_batch     = y_batch.squeeze(-1)             # same for targets\n",
    "\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # ── 2. flatten before extending the lists ───────────────\n",
    "            all_predictions.extend(predictions.cpu().numpy().ravel())\n",
    "            all_targets.extend(y_batch.cpu().numpy().ravel())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, np.array(all_predictions), np.array(all_targets)\n",
    "\n",
    "def get_optimizer_for_model(model: nn.Module, model_type: str, lr: float = 0.001):\n",
    "    \"\"\"Get appropriate optimizer based on model type.\"\"\"\n",
    "    if model_type.lower() == 'transformer':\n",
    "        # AdamW often works better for transformers\n",
    "        return optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    else:\n",
    "        # Standard Adam for other models\n",
    "        return optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def get_learning_rate_scheduler(optimizer, model_type: str, num_training_steps: int = None):\n",
    "    \"\"\"Get appropriate learning rate scheduler based on model type.\"\"\"\n",
    "    if model_type.lower() == 'transformer' and num_training_steps:\n",
    "        # Warmup scheduler for transformers\n",
    "        return optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=optimizer.param_groups[0]['lr'] * 10,\n",
    "            total_steps=num_training_steps,\n",
    "            pct_start=0.1  # 10% warmup\n",
    "        )\n",
    "    else:\n",
    "        # Simple step scheduler for other models\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "\n",
    "def prepare_data_for_model(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    model_type: str,\n",
    "    seq_length: int = 12,\n",
    "    ar_lags: int = 5\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Convert raw arrays to tensors, respecting model expectations.\"\"\"\n",
    "    seq_models = ['lstm', 'transformer', 'tcn', 'temporalconvnet',\n",
    "              'cnn', 'conv1d', 'simpleconvolutional']\n",
    "    if model_type.lower() in seq_models:\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(max(seq_length, ar_lags), len(X)):\n",
    "            feature_seq = X[i - seq_length:i]\n",
    "            ar_returns  = y[i - ar_lags:i].reshape(-1, 1)\n",
    "\n",
    "            ar_pad = np.zeros((seq_length, ar_lags))\n",
    "            ar_pad[-1, :] = ar_returns.ravel()\n",
    "\n",
    "            X_seq.append(np.concatenate([feature_seq, ar_pad], axis=1))\n",
    "            y_seq.append(y[i])\n",
    "\n",
    "        return torch.FloatTensor(np.asarray(X_seq)), torch.FloatTensor(np.asarray(y_seq))\n",
    "\n",
    "    # ── feed‑forward path ───────────────────────────────────────────\n",
    "    X_ff, y_ff = [], []\n",
    "    for i in range(ar_lags, len(X)):\n",
    "        current_features = X[i]\n",
    "        ar_returns       = y[i - ar_lags:i]\n",
    "        X_ff.append(np.concatenate([current_features, ar_returns]))\n",
    "        y_ff.append(y[i])\n",
    "\n",
    "    return torch.FloatTensor(np.asarray(X_ff)), torch.FloatTensor(np.asarray(y_ff))\n",
    "\n",
    "\n",
    "def sp500_training_pipeline(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    dates: pd.DatetimeIndex,\n",
    "    tbill3m: np.ndarray,\n",
    "    model_class: type,\n",
    "    model_kwargs: Dict[str, Any],\n",
    "    model_type: str = 'feedforward',\n",
    "    window_strategy: str = 'rolling',\n",
    "    train_window_years: int = 3,\n",
    "    test_window_years: int = 1,\n",
    "    use_autoencoder: bool = True,\n",
    "    encoding_dim: int = 10,\n",
    "    walk_forward_cv: bool = False,\n",
    "    cv_months: int = 2,\n",
    "    seq_length: int = 12,\n",
    "    ar_lags: int = 5,\n",
    "    epochs: int = 100,\n",
    "    lr: float = 0.001,\n",
    "    batch_size: int = 32,\n",
    "    alpha: float = 0.1,\n",
    "    l1_ratio: float = 0.5,\n",
    "    device: str = 'cpu',\n",
    "    random_seed: int = 42,\n",
    "    plot_results: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Rolling / expanding‑window S&P‑500 forecast pipeline plus TCN support.\"\"\"\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    if not isinstance(dates, pd.DatetimeIndex):\n",
    "        dates = pd.DatetimeIndex(dates)\n",
    "\n",
    "    # === master container =========================================\n",
    "    results: Dict[str, Any] = {\n",
    "        'fold_results': [], 'scalers': [], 'autoencoders': [], 'models': [],\n",
    "        'all_train_predictions': [], 'all_train_targets': [],\n",
    "        'all_test_predictions':  [], 'all_test_targets':  [],\n",
    "        'all_train_dates': [],     'all_test_dates':     [],\n",
    "        'metrics': {\n",
    "            'train_mse': [], 'test_mse': [],\n",
    "            'train_sharpe': [], 'test_sharpe': [],\n",
    "            'train_mae': [], 'test_mae': [],\n",
    "            'train_r2':  [], 'test_r2':  [],\n",
    "            'train_sortino': [], 'test_sortino': [],\n",
    "            'train_hit': [], 'test_hit': []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # --- per‑fold arrays for optional plotting --------------------\n",
    "    fold_predictions, fold_targets, fold_dates = [], [], []\n",
    "\n",
    "    # === build fold timeline ======================================\n",
    "    start_date, end_date = dates[0], dates[-1]\n",
    "    fold_start_dates, cur = [], start_date + pd.DateOffset(years=train_window_years)\n",
    "    while cur + pd.DateOffset(years=test_window_years) <= end_date:\n",
    "        fold_start_dates.append(cur)\n",
    "        cur += pd.DateOffset(years=test_window_years)\n",
    "\n",
    "    print(f\"Starting {window_strategy} window training with {len(fold_start_dates)} folds\")\n",
    "    seq_models = ['lstm', 'transformer', 'tcn', 'temporalconvnet',\n",
    "              'cnn', 'conv1d', 'simpleconvolutional']\n",
    "\n",
    "    # === walk through every fold ==================================\n",
    "    for fold_idx, test_start_date in enumerate(fold_start_dates, 1):\n",
    "        print(f\"\\n— Fold {fold_idx}/{len(fold_start_dates)} —\")\n",
    "        test_end_date  = test_start_date + pd.DateOffset(years=test_window_years)\n",
    "        train_start_date = (test_start_date - pd.DateOffset(years=train_window_years)\n",
    "                            if window_strategy == 'rolling' else start_date)\n",
    "\n",
    "        train_mask = (dates >= train_start_date) & (dates < test_start_date)\n",
    "        test_mask  = (dates >= test_start_date) & (dates < test_end_date)\n",
    "        if train_mask.sum() == 0 or test_mask.sum() == 0:\n",
    "            print(\"Skipping fold – insufficient data\")\n",
    "            continue\n",
    "\n",
    "        # ----------- slice data ----------------------------------\n",
    "        X_train, y_train = X[train_mask], y[train_mask]\n",
    "        X_test,  y_test  = X[test_mask],  y[test_mask]\n",
    "        train_dates, test_dates = dates[train_mask], dates[test_mask]\n",
    "\n",
    "        # ----------- scaling -------------------------------------\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "        # ----------- optional autoencoder ------------------------\n",
    "        autoencoder = None\n",
    "        if use_autoencoder:\n",
    "            autoencoder = train_autoencoder(X_train_scaled, encoding_dim, epochs=50, lr=lr, device=device)\n",
    "            with torch.no_grad():\n",
    "                X_train_scaled = autoencoder.encode(torch.FloatTensor(X_train_scaled).to(device)).cpu().numpy()\n",
    "                X_test_scaled  = autoencoder.encode(torch.FloatTensor(X_test_scaled) .to(device)).cpu().numpy()\n",
    "\n",
    "        # ----------- AR‑lag augmentation -------------------------\n",
    "        X_train_tensor, y_train_tensor = prepare_data_for_model(X_train_scaled, y_train,\n",
    "                                                                model_type, seq_length, ar_lags)\n",
    "        X_test_tensor,  y_test_tensor  = prepare_data_for_model(X_test_scaled,  y_test,\n",
    "                                                                model_type, seq_length, ar_lags)\n",
    "        offset = max(seq_length, ar_lags) if model_type.lower() in seq_models else ar_lags\n",
    "        train_dates_adj, test_dates_adj = train_dates[offset:], test_dates[offset:]\n",
    "\n",
    "        # ----------- model / optimiser ---------------------------\n",
    "        input_dim = X_train_tensor.shape[-1]\n",
    "        model     = model_class(input_dim=input_dim, **model_kwargs).to(device)\n",
    "        criterion = ElasticNetLoss(model=model, alpha=alpha, l1_ratio=l1_ratio)\n",
    "        optimizer = get_optimizer_for_model(model, model_type, lr)\n",
    "        scheduler = get_learning_rate_scheduler(optimizer, model_type)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "        test_loader  = DataLoader(TensorDataset(X_test_tensor,  y_test_tensor),  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # ----------- training loop -------------------------------\n",
    "        print(\"Training final model…\")\n",
    "        final_epochs = epochs // 2 if walk_forward_cv else epochs\n",
    "        for ep in range(final_epochs):\n",
    "            loss = train_model_epoch(model, train_loader, criterion, optimizer, scheduler=scheduler,\n",
    "                                      model_type=model_type, device=device)\n",
    "            if (ep + 1) % 20 == 0:\n",
    "                print(f\"  epoch {ep+1}/{final_epochs}  loss={loss:.6f}\")\n",
    "\n",
    "        # ----------- evaluation ----------------------------------\n",
    "        _, train_predictions, train_targets = evaluate_model(model, train_loader, criterion, device)\n",
    "        _, test_predictions,  test_targets  = evaluate_model(model, test_loader,  criterion, device)\n",
    "\n",
    "        # ----------- metric calc ---------------------------------\n",
    "        train_mse = mean_squared_error(train_targets, train_predictions)\n",
    "        test_mse  = mean_squared_error(test_targets,  test_predictions)\n",
    "        train_mae = mean_absolute_error(train_targets, train_predictions)\n",
    "        test_mae  = mean_absolute_error(test_targets,  test_predictions)\n",
    "        train_r2  = r2_score(train_targets, train_predictions)\n",
    "        test_r2   = r2_score(test_targets,  test_predictions)\n",
    "\n",
    "        train_positions = np.sign(train_predictions)\n",
    "        test_positions  = np.sign(test_predictions)\n",
    "        train_strategy_returns = train_positions * train_targets\n",
    "        test_strategy_returns  = test_positions  * test_targets\n",
    "\n",
    "        train_rf = np.mean(tbill3m[train_mask])\n",
    "        test_rf  = np.mean(tbill3m[test_mask])\n",
    "\n",
    "        train_sharpe  = calculate_sharpe_ratio(train_strategy_returns, train_rf)\n",
    "        test_sharpe   = calculate_sharpe_ratio(test_strategy_returns,  test_rf)\n",
    "        train_sortino = calculate_sortino_ratio(train_strategy_returns, train_rf)\n",
    "        test_sortino  = calculate_sortino_ratio(test_strategy_returns,  test_rf)\n",
    "\n",
    "        train_hit = hit_rate(train_predictions, train_targets)\n",
    "        test_hit  = hit_rate(test_predictions,  test_targets)\n",
    "\n",
    "        print(f\"Train MSE {train_mse:.6f} | MAE {train_mae:.6f} | R² {train_r2:.4f} | Sharpe {train_sharpe:.3f} | Sortino {train_sortino:.3f} | Hit {train_hit:.2%}\")\n",
    "        print(f\"Test  MSE {test_mse:.6f} | MAE {test_mae:.6f} | R² {test_r2:.4f} | Sharpe {test_sharpe:.3f} | Sortino {test_sortino:.3f} | Hit {test_hit:.2%}\")\n",
    "\n",
    "        # Store results - everything below remains the same\n",
    "        fold_result = {\n",
    "            'fold': fold_idx,\n",
    "            'train_period': (train_dates[0], train_dates[-1]),\n",
    "            'test_period': (test_dates[0], test_dates[-1]),\n",
    "            'train_mse': train_mse,\n",
    "            'test_mse': test_mse,\n",
    "            'train_sharpe': train_sharpe,\n",
    "            'test_sharpe': test_sharpe,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'train_sortino': train_sortino,\n",
    "            'test_sortino': test_sortino,\n",
    "            'train_hit': train_hit,\n",
    "            'test_hit': test_hit,\n",
    "            'train_predictions': train_predictions,\n",
    "            'test_predictions': test_predictions,\n",
    "            'train_targets': train_targets,\n",
    "            'test_targets': test_targets,\n",
    "            'train_dates': train_dates_adj,\n",
    "            'test_dates': test_dates_adj\n",
    "        }\n",
    "\n",
    "        results['fold_results'].append(fold_result)\n",
    "        results['scalers'].append(scaler)\n",
    "        results['autoencoders'].append(autoencoder)\n",
    "        results['models'].append(model.state_dict())\n",
    "\n",
    "        # Accumulate for overall plotting\n",
    "        results['all_train_predictions'].extend(train_predictions)\n",
    "        results['all_train_targets'].extend(train_targets)\n",
    "        results['all_test_predictions'].extend(test_predictions)\n",
    "        results['all_test_targets'].extend(test_targets)\n",
    "        results['all_train_dates'].extend(train_dates_adj)\n",
    "        results['all_test_dates'].extend(test_dates_adj)\n",
    "\n",
    "        results['metrics']['train_mse'].append(train_mse)\n",
    "        results['metrics']['test_mse'].append(test_mse)\n",
    "        results['metrics']['train_sharpe'].append(train_sharpe)\n",
    "        results['metrics']['test_sharpe'].append(test_sharpe)\n",
    "        results['metrics']['train_mae'].append(train_mae)\n",
    "        results['metrics']['test_mae'].append(test_mae)\n",
    "        results['metrics']['train_r2'].append(train_r2)\n",
    "        results['metrics']['test_r2'].append(test_r2)\n",
    "        results['metrics']['train_sortino'].append(train_sortino)\n",
    "        results['metrics']['test_sortino'].append(test_sortino)\n",
    "        results['metrics']['train_hit'].append(train_hit)\n",
    "        results['metrics']['test_hit'].append(test_hit)\n",
    "\n",
    "        # Store for individual fold plotting\n",
    "        fold_predictions.append((train_predictions, test_predictions))\n",
    "        fold_targets.append((train_targets, test_targets))\n",
    "        fold_dates.append((train_dates_adj, test_dates_adj))\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    if results['metrics']['train_mse']:\n",
    "        results['overall_metrics'] = {\n",
    "            'avg_train_mse': np.mean(results['metrics']['train_mse']),\n",
    "            'avg_test_mse': np.mean(results['metrics']['test_mse']),\n",
    "            'avg_train_sharpe': np.mean(results['metrics']['train_sharpe']),\n",
    "            'avg_test_sharpe': np.mean(results['metrics']['test_sharpe']),\n",
    "            'std_train_mse': np.std(results['metrics']['train_mse']),\n",
    "            'std_test_mse': np.std(results['metrics']['test_mse']),\n",
    "            'std_train_sharpe': np.std(results['metrics']['train_sharpe']),\n",
    "            'std_test_sharpe': np.std(results['metrics']['test_sharpe']),\n",
    "            'avg_train_mae': np.mean(results['metrics']['train_mae']),\n",
    "            'avg_test_mae': np.mean(results['metrics']['test_mae']),\n",
    "            'std_train_mae': np.std(results['metrics']['train_mae']),\n",
    "            'std_test_mae': np.std(results['metrics']['test_mae']),\n",
    "            'avg_train_r2': np.mean(results['metrics']['train_r2']),\n",
    "            'avg_test_r2': np.mean(results['metrics']['test_r2']),\n",
    "            'std_train_r2': np.std(results['metrics']['train_r2']),\n",
    "            'std_test_r2': np.std(results['metrics']['test_r2']),\n",
    "            'avg_train_sortino': np.mean(results['metrics']['train_sortino']),\n",
    "            'avg_test_sortino': np.mean(results['metrics']['test_sortino']),\n",
    "            'std_train_sortino': np.std(results['metrics']['train_sortino']),\n",
    "            'std_test_sortino': np.std(results['metrics']['test_sortino']),\n",
    "            'avg_train_hit': np.mean(results['metrics']['train_hit']),\n",
    "            'avg_test_hit': np.mean(results['metrics']['test_hit']),\n",
    "            'std_train_hit': np.std(results['metrics']['train_hit']),\n",
    "            'std_test_hit': np.std(results['metrics']['test_hit'])\n",
    "        }\n",
    "\n",
    "        print(\"\\n=== OVERALL RESULTS ===\")\n",
    "        print(f\"Average Train MSE: {results['overall_metrics']['avg_train_mse']:.6f} ± {results['overall_metrics']['std_train_mse']:.6f}\")\n",
    "        print(f\"Average Test MSE: {results['overall_metrics']['avg_test_mse']:.6f} ± {results['overall_metrics']['std_test_mse']:.6f}\")\n",
    "        print(f\"Average Train Sharpe: {results['overall_metrics']['avg_train_sharpe']:.4f} ± {results['overall_metrics']['std_train_sharpe']:.4f}\")\n",
    "        print(f\"Average Test Sharpe: {results['overall_metrics']['avg_test_sharpe']:.4f} ± {results['overall_metrics']['std_test_sharpe']:.4f}\")\n",
    "        print(f\"Average Train MAE: {results['overall_metrics']['avg_train_mae']:.6f} ± {results['overall_metrics']['std_train_mae']:.6f}\")\n",
    "        print(f\"Average Test MAE: {results['overall_metrics']['avg_test_mae']:.6f} ± {results['overall_metrics']['std_test_mae']:.6f}\")\n",
    "        print(f\"Average Train R²: {results['overall_metrics']['avg_train_r2']:.4f} ± {results['overall_metrics']['std_train_r2']:.4f}\")\n",
    "        print(f\"Average Test R²: {results['overall_metrics']['avg_test_r2']:.4f} ± {results['overall_metrics']['std_test_r2']:.4f}\")\n",
    "        print(f\"Average Train Sortino: {results['overall_metrics']['avg_train_sortino']:.4f} ± {results['overall_metrics']['std_train_sortino']:.4f}\")\n",
    "        print(f\"Average Test Sortino: {results['overall_metrics']['avg_test_sortino']:.4f} ± {results['overall_metrics']['std_test_sortino']:.4f}\")\n",
    "        print(f\"Average Train Hit Rate: {results['overall_metrics']['avg_train_hit']:.2%} ± {results['overall_metrics']['std_train_hit']:.2%}\")\n",
    "        print(f\"Average Test Hit Rate: {results['overall_metrics']['avg_test_hit']:.2%} ± {results['overall_metrics']['std_test_hit']:.2%}\")\n",
    "\n",
    "\n",
    "    # Plotting\n",
    "    if plot_results and fold_predictions:\n",
    "        n_plots = len(fold_predictions)\n",
    "        fig, axes = plt.subplots(n_plots, 1, figsize=(15, 4 * n_plots))\n",
    "        if n_plots == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for i, (fold_pred, fold_targ, fold_date) in enumerate(zip(fold_predictions, fold_targets, fold_dates)):\n",
    "            train_pred, test_pred = fold_pred\n",
    "            train_targ, test_targ = fold_targ\n",
    "            train_date, test_date = fold_date\n",
    "\n",
    "            ax = axes[i]\n",
    "\n",
    "            # Plot actual returns (always black)\n",
    "            ax.plot(train_date, train_targ, 'k-', label='Actual Returns', linewidth=1)\n",
    "            ax.plot(test_date, test_targ, 'k-', linewidth=1)\n",
    "\n",
    "            # Plot predictions (blue for train, red for test)\n",
    "            ax.plot(train_date, train_pred, 'b-', label='Train Predictions', alpha=0.7)\n",
    "            ax.plot(test_date, test_pred, 'r-', label='Test Predictions', alpha=0.7)\n",
    "\n",
    "            # Add vertical line to separate train/test\n",
    "            if len(test_date) > 0:\n",
    "                ax.axvline(x=test_date[0], color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "            ax.set_title(f'Fold {i+1}: Predictions vs Actual Returns')\n",
    "            ax.set_ylabel('Returns')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Overall performance plot\n",
    "        if len(results['all_train_dates']) > 0:\n",
    "            plt.figure(figsize=(15, 6))\n",
    "\n",
    "            # Combine and sort all data by date\n",
    "            all_data = []\n",
    "            for train_pred, train_targ, train_date in zip(\n",
    "                [results['fold_results'][i]['train_predictions'] for i in range(len(results['fold_results']))],\n",
    "                [results['fold_results'][i]['train_targets'] for i in range(len(results['fold_results']))],\n",
    "                [results['fold_results'][i]['train_dates'] for i in range(len(results['fold_results']))]\n",
    "            ):\n",
    "                for pred, targ, date in zip(train_pred, train_targ, train_date):\n",
    "                    all_data.append((date, targ, pred, 'train'))\n",
    "\n",
    "            for test_pred, test_targ, test_date in zip(\n",
    "                [results['fold_results'][i]['test_predictions'] for i in range(len(results['fold_results']))],\n",
    "                [results['fold_results'][i]['test_targets'] for i in range(len(results['fold_results']))],\n",
    "                [results['fold_results'][i]['test_dates'] for i in range(len(results['fold_results']))]\n",
    "            ):\n",
    "                for pred, targ, date in zip(test_pred, test_targ, test_date):\n",
    "                    all_data.append((date, targ, pred, 'test'))\n",
    "\n",
    "            # Sort by date\n",
    "            all_data.sort(key=lambda x: x[0])\n",
    "\n",
    "            # Separate data\n",
    "            dates_all = [x[0] for x in all_data]\n",
    "            targets_all = [x[1] for x in all_data]\n",
    "            predictions_all = [x[2] for x in all_data]\n",
    "            types_all = [x[3] for x in all_data]\n",
    "\n",
    "            # Plot\n",
    "            plt.plot(dates_all, targets_all, 'k-', label='Actual Returns', linewidth=1)\n",
    "\n",
    "            # Plot predictions with different colors\n",
    "            train_mask = np.array(types_all) == 'train'\n",
    "            test_mask = np.array(types_all) == 'test'\n",
    "\n",
    "            if np.any(train_mask):\n",
    "                plt.plot(np.array(dates_all)[train_mask], np.array(predictions_all)[train_mask],\n",
    "                        'b.', label='Train Predictions', alpha=0.7, markersize=3)\n",
    "            if np.any(test_mask):\n",
    "                plt.plot(np.array(dates_all)[test_mask], np.array(predictions_all)[test_mask],\n",
    "                        'r.', label='Test Predictions', alpha=0.7, markersize=3)\n",
    "\n",
    "            plt.title('Overall Model Performance: Predictions vs Actual Returns')\n",
    "            plt.ylabel('Returns')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7a95400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T15:31:17.228082Z",
     "start_time": "2025-06-20T15:31:17.149397Z"
    }
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"data_non_std.csv\",parse_dates=[\"Unnamed: 0\"]).rename(columns={'Unnamed: 0': 'Date'}) \n",
    "features = df.drop(columns=[\"returns\", \"Date\"])\n",
    "target = df[\"returns\"].values.astype(np.float32)\n",
    "dates = pd.to_datetime(df[\"Date\"]).astype('datetime64[ns]').tolist()\n",
    "tbill3m_data = df[\"tbill3m\"].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a61acb33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T12:58:48.457477Z",
     "start_time": "2025-06-20T12:58:45.697841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting rolling window training with 13 folds\n",
      "\n",
      "— Fold 1/13 —\n",
      "Training final model…\n",
      "1. Initial input shape: torch.Size([32, 11])\n",
      "2. After unsqueeze shape: torch.Size([32, 11, 1])\n",
      "3. input_projection expects input size: 11\n",
      "   input_projection outputs size: 64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (352x1 and 11x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 142\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m    140\u001b[0m tbill3m_data \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtbill3m\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 142\u001b[0m results_tcn \u001b[38;5;241m=\u001b[39m \u001b[43msp500_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtbill3m\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtbill3m_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m   \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTemporalFusionTransformer2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemporalfusiontransformer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# or 'simpleconvolutional'\u001b[39;49;00m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_heads\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrolling\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_window_years\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_window_years\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_autoencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mar_lags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"model_kwargs={\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m        'num_channels':[32, 64, 32], 'kernel_size':5, 'dropout':0.25\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    }\"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 326\u001b[0m, in \u001b[0;36msp500_training_pipeline\u001b[0;34m(X, y, dates, tbill3m, model_class, model_kwargs, model_type, window_strategy, train_window_years, test_window_years, use_autoencoder, encoding_dim, walk_forward_cv, cv_months, seq_length, ar_lags, epochs, lr, batch_size, alpha, l1_ratio, device, random_seed, plot_results)\u001b[0m\n\u001b[1;32m    324\u001b[0m final_epochs \u001b[38;5;241m=\u001b[39m epochs \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m walk_forward_cv \u001b[38;5;28;01melse\u001b[39;00m epochs\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(final_epochs):\n\u001b[0;32m--> 326\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (ep \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 92\u001b[0m, in \u001b[0;36mtrain_model_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, scheduler, model_type, device)\u001b[0m\n\u001b[1;32m     90\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     91\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 92\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     93\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, y_batch)\n\u001b[1;32m     94\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/volatilityenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/volatilityenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/GitHub/SQTgit/TM_models.py:283\u001b[0m, in \u001b[0;36mTemporalFusionTransformer2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. input_projection expects input size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_projection\u001b[38;5;241m.\u001b[39min_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   input_projection outputs size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_projection\u001b[38;5;241m.\u001b[39mout_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4. After input_projection shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Positional encoding so model knows timestep\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/volatilityenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/volatilityenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/volatilityenv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (352x1 and 11x64)"
     ]
    }
   ],
   "source": [
    "class SimpleFeedForward(nn.Module):\n",
    "    \"\"\"Example feedforward model\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=200, dropout=0):\n",
    "        super(SimpleFeedForward, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"Example LSTM model\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=200, num_layers=2, dropout=0):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(dropout),\n",
    "                                nn.Linear(hidden_dim // 2, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])  # Use last time step output\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 model_dim: int = 128,   # divisible by nhead\n",
    "                 nhead: int = 8,\n",
    "                 num_layers: int = 2,\n",
    "                 dropout: float = 0.1,\n",
    "                 max_seq_length: int = 500):  # for positional encoding\n",
    "        super().__init__()\n",
    "\n",
    "        # project raw features to model_dim\n",
    "        self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "        \n",
    "        # Add positional encoding \n",
    "        self.pos_encoding = PositionalEncoding(model_dim, dropout, max_seq_length)\n",
    "\n",
    "        # vanilla encoder stack\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=model_dim,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=model_dim * 4,  # standard practice\n",
    "                dropout=dropout,\n",
    "                activation='gelu',  # often works better than relu for transformers\n",
    "                batch_first=True        # so x is (B, T, F)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # regression head with additional processing\n",
    "        self.norm = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(model_dim, 1)\n",
    "\n",
    "    def forward(self, x):               # x: (B, T, input_dim)\n",
    "        x = self.input_proj(x)          # (B, T, model_dim)\n",
    "        x = self.pos_encoding(x)        # Add positional information\n",
    "        x = self.encoder(x)             # (B, T, model_dim)\n",
    "        x = self.norm(x[:, -1])         # Layer norm on last time step\n",
    "        x = self.dropout(x)             # Additional dropout\n",
    "        return self.fc(x)               # (B, 1)\n",
    "\n",
    "class SimpleConvolutional(nn.Module):\n",
    "    \"\"\"\n",
    "    1-D CNN for sequence-to-one forecasting.\n",
    "    Assumes input tensor shape: (batch, seq_len, n_features)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,               # -- n_features after any AR-lag concat\n",
    "        num_channels: List[int] = [32, 64, 32],\n",
    "        kernel_size: int = 5,\n",
    "        dropout: float = 0.25,\n",
    "        seq_length: int = 12          # 〈NEW – keep default same as pipeline〉\n",
    "    ):\n",
    "        super(SimpleConvolutional, self).__init__()\n",
    "\n",
    "        layers, in_channels = [], input_dim\n",
    "        for out_channels in num_channels:\n",
    "            layers += [\n",
    "                nn.Conv1d(in_channels, out_channels,\n",
    "                          kernel_size, padding=kernel_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "        # ── force length-1 feature map so fc dim is invariant to seq_length ──\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)                   # ← NEW\n",
    "        self.fc   = nn.Linear(in_channels, 1)                 # 〈in_channels == last out_channels〉\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, n_features)\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, 2)          # (batch, n_features, seq_len)\n",
    "        x = self.conv(x)               # (batch, channels, L)\n",
    "        x = self.pool(x)               # (batch, channels, 1)\n",
    "        x = x.view(x.size(0), -1)      # (batch, channels)\n",
    "        return self.fc(x).squeeze(-1)  # (batch,)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.transpose(0, 1)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "tbill3m_data = df[\"tbill3m\"].values.astype(np.float32)\n",
    "\n",
    "results_tcn = sp500_training_pipeline(\n",
    "    X=features,\n",
    "    y=target,\n",
    "    dates=dates,\n",
    "    tbill3m=tbill3m_data,\n",
    "   model_class  = TemporalFusionTransformer2,\n",
    "    model_type   = 'temporalfusiontransformer',                    # or 'simpleconvolutional'\n",
    "    model_kwargs = {\n",
    "        'hidden_dim': 64, 'num_heads': 4, 'num_layers': 2, 'dropout': 0.1,\n",
    "    },\n",
    "    window_strategy='rolling',\n",
    "    train_window_years=3,\n",
    "    test_window_years=1,\n",
    "    use_autoencoder=True,\n",
    "    encoding_dim=10,\n",
    "    seq_length=24,\n",
    "    ar_lags=1,\n",
    "    batch_size=32,\n",
    "    epochs=80,\n",
    "    lr=0.001,\n",
    "    plot_results=True,\n",
    "    alpha=0.0,\n",
    "    l1_ratio=0.0,\n",
    ")\n",
    "\n",
    "\"\"\"model_kwargs={\n",
    "        'num_channels':[32, 64, 32], 'kernel_size':5, 'dropout':0.25\n",
    "    }\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ae3a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T12:45:40.347487Z",
     "start_time": "2025-06-20T12:45:40.340766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Imported object : <module 'sympy' from 'C:\\\\Users\\\\miklo\\\\PycharmProjects\\\\SQTgit\\\\venv\\\\Lib\\\\site-packages\\\\sympy\\\\__init__.py'>\n",
      "• Version         : 1.14.0\n",
      "• File            : C:\\Users\\miklo\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\sympy\\__init__.py\n",
      "• printing attr?  : True\n"
     ]
    }
   ],
   "source": [
    "# import importlib, sys\n",
    "# sym = importlib.import_module(\"sympy\")\n",
    "# print(\"• Imported object :\", sym)\n",
    "# print(\"• Version         :\", getattr(sym, \"__version__\", \" <no __version__>\"))\n",
    "# print(\"• File            :\", getattr(sym, \"__file__\", \" <no __file__>\"))\n",
    "# print(\"• printing attr?  :\", hasattr(sym, \"printing\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1115a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T14:58:19.259579Z",
     "start_time": "2025-06-20T14:58:11.057394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting rolling window training with 13 folds\n",
      "\n",
      "— Fold 1/13 —\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x0000023A34F0DE80>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\miklo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "  File \"C:\\Users\\miklo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\threading.py\", line 1477, in enumerate\n",
      "    def enumerate():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results_tft \u001b[38;5;241m=\u001b[39m \u001b[43msp500_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtbill3m\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtbill3m_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTemporalFusionTransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_heads\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use 'tft' as model_type\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrolling\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_window_years\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_window_years\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_autoencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TFT can handle more complex features\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mar_lags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Lower learning rate for TFT\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 299\u001b[0m, in \u001b[0;36msp500_training_pipeline\u001b[1;34m(X, y, dates, tbill3m, model_class, model_kwargs, model_type, window_strategy, train_window_years, test_window_years, use_autoencoder, encoding_dim, walk_forward_cv, cv_months, seq_length, ar_lags, epochs, lr, batch_size, alpha, l1_ratio, device, random_seed, plot_results)\u001b[0m\n\u001b[0;32m    297\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_autoencoder:\n\u001b[1;32m--> 299\u001b[0m     autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    301\u001b[0m         X_train_scaled \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39mencode(torch\u001b[38;5;241m.\u001b[39mFloatTensor(X_train_scaled)\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[1;32mIn[1], line 63\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[1;34m(X_train, encoding_dim, epochs, lr, device)\u001b[0m\n\u001b[0;32m     61\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m AutoEncoder(input_dim, encoding_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     62\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m---> 63\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m X_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(X_train)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     66\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(X_tensor)\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\optim\\adam.py:100\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused, decoupled_weight_decay)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor betas[1] must be 1-element\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m     88\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m     89\u001b[0m     betas\u001b[38;5;241m=\u001b[39mbetas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m     decoupled_weight_decay\u001b[38;5;241m=\u001b[39mdecoupled_weight_decay,\n\u001b[0;32m     99\u001b[0m )\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:369\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    366\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: param_groups}]\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\_compile.py:46\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m disable_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dynamo_disable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m disable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     49\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\_dynamo\\__init__.py:53\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m graph_break_reasons, guard_failures, orig_code_map, reset_frame_count\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Register polyfill functions\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolyfills\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loader \u001b[38;5;28;01mas\u001b[39;00m _  \u001b[38;5;66;03m# usort: skip # noqa: F401\u001b[39;00m\n\u001b[0;32m     56\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_in_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massume_constant_result\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     83\u001b[0m ]\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# allowlist this for weights_only load of NJTs\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\_dynamo\\polyfills\\loader.py:25\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# See also the TYPE_CHECKING block in torch/_dynamo/polyfills/__init__.py\u001b[39;00m\n\u001b[0;32m     15\u001b[0m POLYFILLED_MODULE_NAMES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltins\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctools\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 25\u001b[0m POLYFILLED_MODULES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msubmodule\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolyfills\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPOLYFILLED_MODULE_NAMES\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Unregister the builtin functions from _builtin_function_ids to let them to be\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# dispatched with the appropriate VariableTracker type. Otherwise, they will be\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# dispatched with BuiltinVariable if present in _builtin_function_ids.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m polyfill_module \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULES:\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\_dynamo\\polyfills\\loader.py:26\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# See also the TYPE_CHECKING block in torch/_dynamo/polyfills/__init__.py\u001b[39;00m\n\u001b[0;32m     15\u001b[0m POLYFILLED_MODULE_NAMES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltins\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctools\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m POLYFILLED_MODULES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msubmodule\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolyfills\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m submodule \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULE_NAMES\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Unregister the builtin functions from _builtin_function_ids to let them to be\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# dispatched with the appropriate VariableTracker type. Otherwise, they will be\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# dispatched with BuiltinVariable if present in _builtin_function_ids.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m polyfill_module \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULES:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     87\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\_dynamo\\polyfills\\builtins.py:30\u001b[0m\n\u001b[0;32m     19\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menumerate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m ]\n\u001b[0;32m     27\u001b[0m _T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_T\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;129;43m@substitute_in_graph\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_constant_fold_through\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\_dynamo\\decorators.py:427\u001b[0m, in \u001b[0;36msubstitute_in_graph.<locals>.wrapper\u001b[1;34m(traceable_fn)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(original_fn) \u001b[38;5;129;01min\u001b[39;00m _polyfilled_function_ids:\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate polyfilled object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 427\u001b[0m rule_map: \u001b[38;5;28mdict\u001b[39m[Any, \u001b[38;5;28mtype\u001b[39m[VariableTracker]] \u001b[38;5;241m=\u001b[39m \u001b[43mget_torch_obj_rule_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_fn \u001b[38;5;129;01min\u001b[39;00m rule_map:\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    430\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with different rules: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPolyfilledFunctionVariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrule_map[original_fn]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m     )\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py:2870\u001b[0m, in \u001b[0;36mget_torch_obj_rule_map\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2868\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mitems():  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   2869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k:\n\u001b[1;32m-> 2870\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mload_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2871\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2872\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _module_dir(torch) \u001b[38;5;241m+\u001b[39m k[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch/\u001b[39m\u001b[38;5;124m\"\u001b[39m) :]\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py:2901\u001b[0m, in \u001b[0;36mload_object\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m   2899\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2900\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid obj name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2901\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[43m_load_obj_from_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2902\u001b[0m     val \u001b[38;5;241m=\u001b[39m unwrap_if_wrapper(val)\n\u001b[0;32m   2903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\_dynamo\\trace_rules.py:2885\u001b[0m, in \u001b[0;36m_load_obj_from_str\u001b[1;34m(fully_qualified_name)\u001b[0m\n\u001b[0;32m   2883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load_obj_from_str\u001b[39m(fully_qualified_name):\n\u001b[0;32m   2884\u001b[0m     module, obj_name \u001b[38;5;241m=\u001b[39m fully_qualified_name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 2885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m, obj_name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     87\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\_higher_order_ops\\map.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DispatchKey\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m suspend_functionalization\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maot_autograd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AOTConfig, create_joint\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_higher_order_ops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     _has_potential_branch_input_alias,\n\u001b[0;32m      9\u001b[0m     _has_potential_branch_input_mutation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     UnsupportedAliasMutationException,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HigherOrderOperator\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py:135\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aot_autograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraced_function_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     aot_dispatch_subclass,\n\u001b[0;32m    114\u001b[0m     create_functional_call,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m     fn_prepped_for_autograd,\n\u001b[0;32m    120\u001b[0m )\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aot_autograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     _get_autocast_states,\n\u001b[0;32m    123\u001b[0m     _get_symint_hints,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m     strict_zip,\n\u001b[0;32m    134\u001b[0m )\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartitioners\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m default_partition\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mzip\u001b[39m \u001b[38;5;241m=\u001b[39m strict_zip\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# This global counter increments every time we compile a graph with\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# AOTAutograd.  You can use this to correlate runtime error messages\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# with compile time (e.g., if you get an error at runtime saying\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# one counter is allocated per entire compiled block (but this block\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# may involve compiling multiple subgraphs; e.g., for forwards/backwards)\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\_functorch\\partitioners.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CheckpointPolicy\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_activation_checkpointing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_info_provider\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphInfoProvider\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_activation_checkpointing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mknapsack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m     dp_knapsack,\n\u001b[0;32m     40\u001b[0m     greedy_knapsack,\n\u001b[0;32m     41\u001b[0m     ilp_knapsack,\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_activation_checkpointing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mknapsack_evaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KnapsackEvaluator\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\_functorch\\_activation_checkpointing\\graph_info_provider.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Optional\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Graph, Node\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mGraphInfoProvider\u001b[39;00m:\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\networkx\\__init__.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_imports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _lazy_import\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _clear_cache, _dispatchable\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# load_and_call entry_points, set configs\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\networkx\\utils\\__init__.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mheaps\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\networkx\\utils\\backends.py:94\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rv\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Note: \"networkx\" is in `backend_info` but ignored in `backends` and `config.backends`.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# It is valid to use \"networkx\" as a backend argument and in `config.backend_priority`.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# If we make \"networkx\" a \"proper\" backend, put it in `backends` and `config.backends`.\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m backends \u001b[38;5;241m=\u001b[39m \u001b[43m_get_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnetworkx.backends\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Use _set_configs_from_environment() below to fill backend_info dict as\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# the last step in importing networkx\u001b[39;00m\n\u001b[0;32m     98\u001b[0m backend_info \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\networkx\\utils\\backends.py:67\u001b[0m, in \u001b[0;36m_get_backends\u001b[1;34m(group, load_and_call)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_backends\u001b[39m(group, \u001b[38;5;241m*\u001b[39m, load_and_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    Retrieve NetworkX ``backends`` and ``backend_info`` from the entry points.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    A warning is displayed if an error occurs while loading a backend.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[43mentry_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     rv \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\metadata\\__init__.py:1011\u001b[0m, in \u001b[0;36mentry_points\u001b[1;34m(**params)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return EntryPoint objects for all installed packages.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \n\u001b[0;32m   1002\u001b[0m \u001b[38;5;124;03mPass selection parameters (group or name) to filter the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m:return: EntryPoints for all installed packages.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m eps \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   1009\u001b[0m     dist\u001b[38;5;241m.\u001b[39mentry_points \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m _unique(distributions())\n\u001b[0;32m   1010\u001b[0m )\n\u001b[1;32m-> 1011\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEntryPoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\metadata\\__init__.py:1009\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mentry_points\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EntryPoints:\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return EntryPoint objects for all installed packages.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \n\u001b[0;32m   1002\u001b[0m \u001b[38;5;124;03m    Pass selection parameters (group or name) to filter the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m    :return: EntryPoints for all installed packages.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m     eps \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 1009\u001b[0m         \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_points\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m _unique(distributions())\n\u001b[0;32m   1010\u001b[0m     )\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EntryPoints(eps)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\metadata\\__init__.py:496\u001b[0m, in \u001b[0;36mDistribution.entry_points\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mentry_points\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EntryPoints:\n\u001b[0;32m    490\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;124;03m    Return EntryPoints for this distribution.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m    Custom providers may provide the ``entry_points.txt`` file\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;124;03m    or override this property.\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EntryPoints\u001b[38;5;241m.\u001b[39m_from_text_for(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentry_points.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\metadata\\__init__.py:915\u001b[0m, in \u001b[0;36mPathDistribution.read_text\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\n\u001b[0;32m    909\u001b[0m         \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m,\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;167;01mPermissionError\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m     ):\n\u001b[1;32m--> 915\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoinpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py:546\u001b[0m, in \u001b[0;36mPath.read_text\u001b[1;34m(self, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;66;03m# Call io.text_encoding() here to ensure any warning is raised at an\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# appropriate stack level.\u001b[39;00m\n\u001b[0;32m    545\u001b[0m encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m--> 546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPathBase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_abc.py:632\u001b[0m, in \u001b[0;36mPathBase.read_text\u001b[1;34m(self, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    629\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;124;03m    Open the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 632\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pathlib\\_local.py:537\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    536\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m--> 537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen codecs>:312\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_tft = sp500_training_pipeline(\n",
    "    X=features,\n",
    "    y=target,\n",
    "    dates=dates,\n",
    "    tbill3m=tbill3m_data,\n",
    "    model_class=TemporalFusionTransformer,\n",
    "    model_kwargs={\n",
    "        'hidden_dim': 128,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.1\n",
    "    },\n",
    "    model_type='tft',  # Use 'tft' as model_type\n",
    "    window_strategy='rolling',\n",
    "    train_window_years=3,\n",
    "    test_window_years=1,\n",
    "    use_autoencoder=True,\n",
    "    encoding_dim=15,  # TFT can handle more complex features\n",
    "    seq_length=18,\n",
    "    ar_lags=5,\n",
    "    epochs=10,\n",
    "    lr=0.0005,  # Lower learning rate for TFT\n",
    "    plot_results=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5333af2ea8b90667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T15:31:38.778723Z",
     "start_time": "2025-06-20T15:31:20.945181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting rolling window training with 13 folds\n",
      "\n",
      "— Fold 1/13 —\n",
      "Training final model…\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x32 and 128x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results_tft \u001b[38;5;241m=\u001b[39m \u001b[43msp500_training_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtbill3m\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtbill3m_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTemporalFusionTransformer2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_heads\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdropout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use 'tft' as model_type\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwindow_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrolling\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_window_years\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_window_years\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_autoencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TFT can handle more complex features\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mar_lags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Lower learning rate for TFT\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 326\u001b[0m, in \u001b[0;36msp500_training_pipeline\u001b[1;34m(X, y, dates, tbill3m, model_class, model_kwargs, model_type, window_strategy, train_window_years, test_window_years, use_autoencoder, encoding_dim, walk_forward_cv, cv_months, seq_length, ar_lags, epochs, lr, batch_size, alpha, l1_ratio, device, random_seed, plot_results)\u001b[0m\n\u001b[0;32m    324\u001b[0m final_epochs \u001b[38;5;241m=\u001b[39m epochs \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m walk_forward_cv \u001b[38;5;28;01melse\u001b[39;00m epochs\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(final_epochs):\n\u001b[1;32m--> 326\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (ep \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 92\u001b[0m, in \u001b[0;36mtrain_model_epoch\u001b[1;34m(model, dataloader, criterion, optimizer, scheduler, model_type, device)\u001b[0m\n\u001b[0;32m     90\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     91\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 92\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     93\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, y_batch)\n\u001b[0;32m     94\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\TM_models.py:283\u001b[0m, in \u001b[0;36mTemporalFusionTransformer2.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    281\u001b[0m batch_size, seq_len \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    282\u001b[0m static \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 283\u001b[0m static_enriched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatic_enrichment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m static_broadcasted \u001b[38;5;241m=\u001b[39m static_enriched\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(batch_size, seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\n\u001b[0;32m    285\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m static_broadcasted\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\PycharmProjects\\SQTgit\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x32 and 128x128)"
     ]
    }
   ],
   "source": [
    "results_tft = sp500_training_pipeline(\n",
    "    X=features,\n",
    "    y=target,\n",
    "    dates=dates,\n",
    "    tbill3m=tbill3m_data,\n",
    "    model_class=TemporalFusionTransformer2,\n",
    "    model_kwargs={\n",
    "        'hidden_dim': 128,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.1\n",
    "    },\n",
    "    model_type='tft',  # Use 'tft' as model_type\n",
    "    window_strategy='rolling',\n",
    "    train_window_years=3,\n",
    "    test_window_years=1,\n",
    "    use_autoencoder=True,\n",
    "    encoding_dim=15,  # TFT can handle more complex features\n",
    "    seq_length=18,\n",
    "    ar_lags=5,\n",
    "    epochs=80,\n",
    "    lr=0.0005,  # Lower learning rate for TFT\n",
    "    plot_results=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (volatilityenv)",
   "language": "python",
   "name": "volatilityenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
