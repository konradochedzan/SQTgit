{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0614ffed",
   "metadata": {},
   "source": [
    "# READ ME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea50809",
   "metadata": {},
   "source": [
    "This is a training pipeline as of 19.06.2025 where we start to include more complex predictive models in our project. First, we introduce the Temporal Fusion Transformer (TFT) and Temporal ConvNet (TCN) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3890e176",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:15:03.439600Z",
     "start_time": "2025-06-18T20:15:03.379956Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "import warnings\n",
    "import random\n",
    "from Base_models import FeedForwardPredictor, AutoEncoder, ElasticNetLoss, SharpeRatioLoss\n",
    "from TM_models import TemporalConvNet, TemporalFusionTransformer, TemporalFusionTransformer2\n",
    "import math\n",
    "import itertools\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # might slow down but ensures reproducibility\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed()\n",
    "\n",
    "def calculate_sharpe_ratio(strategy_returns: np.ndarray, rf_rate: float) -> float:\n",
    "    \"\"\"Calculate annualized Sharpe ratio using strategy returns and actual risk-free rate\"\"\"\n",
    "    if len(strategy_returns) == 0 or np.std(strategy_returns) == 0:\n",
    "        return 0.0\n",
    "    # rf_rate is already annual, convert to daily\n",
    "    daily_rf = rf_rate / 252  # Assuming daily data\n",
    "    excess_returns = strategy_returns - daily_rf\n",
    "    return np.sqrt(252) * np.mean(excess_returns) / np.std(strategy_returns)\n",
    "\n",
    "def calculate_sortino_ratio(strategy_returns: np.ndarray, rf_rate: float) -> float:\n",
    "    \"\"\"Calculate annualized Sortino ratio using strategy returns and actual risk-free rate\"\"\"\n",
    "    if len(strategy_returns) == 0:\n",
    "        return 0.0\n",
    "    # rf_rate is already annual, convert to daily\n",
    "    daily_rf = rf_rate / 252  # Assuming daily data\n",
    "    excess_returns = strategy_returns - daily_rf\n",
    "    downside_returns = excess_returns[excess_returns < 0]\n",
    "    if len(downside_returns) == 0:\n",
    "        return np.inf\n",
    "    return np.sqrt(252) * np.mean(excess_returns) / np.std(downside_returns)\n",
    "\n",
    "def hit_rate(pred: np.ndarray, targ: np.ndarray) -> float:\n",
    "    \"\"\"Percentage of months the sign is predicted correctly.\"\"\"\n",
    "    return np.mean(np.sign(pred) == np.sign(targ))\n",
    "\n",
    "def train_autoencoder(X_train: np.ndarray, encoding_dim: int, epochs: int = 100, \n",
    "                     lr: float = 0.001, device: str = 'cpu') -> AutoEncoder:\n",
    "    \"\"\"Train autoencoder on training data only\"\"\"\n",
    "    input_dim = X_train.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    dataset = TensorDataset(X_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    autoencoder.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            x_batch = batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(x_batch)\n",
    "            loss = criterion(reconstructed, x_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    autoencoder.eval()\n",
    "    return autoencoder\n",
    "\n",
    "def train_model_epoch(model: nn.Module, dataloader: DataLoader, criterion: nn.Module,\n",
    "                     optimizer: optim.Optimizer, scheduler=None, model_type: str = 'feedforward', device: str = 'cpu') -> float:\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch).squeeze()\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        if model_type.lower() == 'transformer':\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        if scheduler and isinstance(scheduler, optim.lr_scheduler.OneCycleLR):\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# def evaluate_model(model: nn.Module, dataloader: DataLoader, criterion: nn.Module,\n",
    "#                   device: str) -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "#     \"\"\"Evaluate model and return loss, predictions, and targets\"\"\"\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     all_predictions = []\n",
    "#     all_targets = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for X_batch, y_batch in dataloader:\n",
    "#             X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#             predictions = model(X_batch).squeeze()\n",
    "#             loss = criterion(predictions, y_batch)\n",
    "#             total_loss += loss.item()\n",
    "#             all_predictions.extend(predictions.cpu().numpy())\n",
    "#             all_targets.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "#     return total_loss / len(dataloader), np.array(all_predictions), np.array(all_targets)\n",
    "\n",
    "def evaluate_model(model: nn.Module,\n",
    "                   dataloader: DataLoader,\n",
    "                   criterion: nn.Module,\n",
    "                   device: str) -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_predictions, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # ── 1. keep the batch axis ───────────────────────────────\n",
    "            predictions = model(X_batch).squeeze(-1)      # shape: (batch_size,)\n",
    "            y_batch     = y_batch.squeeze(-1)             # same for targets\n",
    "\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # ── 2. flatten before extending the lists ───────────────\n",
    "            all_predictions.extend(predictions.cpu().numpy().ravel())\n",
    "            all_targets.extend(y_batch.cpu().numpy().ravel())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, np.array(all_predictions), np.array(all_targets)\n",
    "\n",
    "def get_optimizer_for_model(model: nn.Module, model_type: str, lr: float = 0.001):\n",
    "    \"\"\"Get appropriate optimizer based on model type.\"\"\"\n",
    "    if model_type.lower() in ['transformer', 'temporalfusiontransformer']:\n",
    "        # AdamW often works better for transformers\n",
    "        return optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "    else:\n",
    "        # Standard Adam for other models\n",
    "        return optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def get_learning_rate_scheduler(optimizer, model_type: str, num_training_steps: int = None):\n",
    "    \"\"\"Get appropriate learning rate scheduler based on model type.\"\"\"\n",
    "    if model_type.lower() == 'transformer' and num_training_steps:\n",
    "        # Warmup scheduler for transformers\n",
    "        return optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, \n",
    "            max_lr=optimizer.param_groups[0]['lr'] * 10,\n",
    "            total_steps=num_training_steps,\n",
    "            pct_start=0.1  # 10% warmup\n",
    "        )\n",
    "    else:\n",
    "        # Simple step scheduler for other models\n",
    "        return optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "\n",
    "def prepare_data_for_model(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    model_type: str,\n",
    "    seq_length: int = 12,\n",
    "    ar_lags: int = 5\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Convert raw arrays to tensors, respecting model expectations.\"\"\"\n",
    "    seq_models = ['lstm', 'transformer', 'tcn', 'temporalconvnet',\n",
    "              'cnn', 'conv1d', 'simpleconvolutional','temporalfusiontransformer']\n",
    "    if model_type.lower() in seq_models:\n",
    "        X_seq, y_seq = [], []\n",
    "        for i in range(max(seq_length, ar_lags), len(X)):\n",
    "            feature_seq = X[i - seq_length:i]\n",
    "            ar_returns  = y[i - ar_lags:i].reshape(-1, 1)\n",
    "\n",
    "            ar_pad = np.zeros((seq_length, ar_lags))\n",
    "            ar_pad[-1, :] = ar_returns.ravel()\n",
    "\n",
    "            X_seq.append(np.concatenate([feature_seq, ar_pad], axis=1))\n",
    "            y_seq.append(y[i])\n",
    "\n",
    "        return torch.FloatTensor(np.asarray(X_seq)), torch.FloatTensor(np.asarray(y_seq))\n",
    "\n",
    "    # ── feed‑forward path ───────────────────────────────────────────\n",
    "    X_ff, y_ff = [], []\n",
    "    for i in range(ar_lags, len(X)):\n",
    "        current_features = X[i]\n",
    "        ar_returns       = y[i - ar_lags:i]\n",
    "        X_ff.append(np.concatenate([current_features, ar_returns]))\n",
    "        y_ff.append(y[i])\n",
    "\n",
    "    return torch.FloatTensor(np.asarray(X_ff)), torch.FloatTensor(np.asarray(y_ff))\n",
    "\n",
    "\n",
    "def sp500_training_pipeline(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    dates: pd.DatetimeIndex,\n",
    "    tbill3m: np.ndarray,\n",
    "    model_class: type,\n",
    "    model_kwargs: Dict[str, Any],\n",
    "    model_type: str = 'feedforward',\n",
    "    window_strategy: str = 'rolling',\n",
    "    train_window_years: int = 3,\n",
    "    test_window_years: int = 1,\n",
    "    use_autoencoder: bool = True,\n",
    "    encoding_dim: int = 10,\n",
    "    walk_forward_cv: bool = False,\n",
    "    cv_months: int = 2,\n",
    "    seq_length: int = 12,\n",
    "    ar_lags: int = 5,\n",
    "    epochs: int = 100,\n",
    "    lr: float = 0.001,\n",
    "    batch_size: int = 32,\n",
    "    alpha: float = 0.1,\n",
    "    l1_ratio: float = 0.5,\n",
    "    device: str = 'cpu',\n",
    "    random_seed: int = 42,\n",
    "    plot_results: bool = True,\n",
    "    do_print = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Rolling / expanding‑window S&P‑500 forecast pipeline plus TCN support.\"\"\"\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    if not isinstance(dates, pd.DatetimeIndex):\n",
    "        dates = pd.DatetimeIndex(dates)\n",
    "\n",
    "    # === master container =========================================\n",
    "    results: Dict[str, Any] = {\n",
    "        'fold_results': [], 'scalers': [], 'autoencoders': [], 'models': [],\n",
    "        'all_train_predictions': [], 'all_train_targets': [],\n",
    "        'all_test_predictions':  [], 'all_test_targets':  [],\n",
    "        'all_train_dates': [],     'all_test_dates':     [],\n",
    "        'metrics': {\n",
    "            'train_mse': [], 'test_mse': [],\n",
    "            'train_sharpe': [], 'test_sharpe': [],\n",
    "            'train_mae': [], 'test_mae': [],\n",
    "            'train_r2':  [], 'test_r2':  [],\n",
    "            'train_sortino': [], 'test_sortino': [],\n",
    "            'train_hit': [], 'test_hit': []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # --- per‑fold arrays for optional plotting --------------------\n",
    "    fold_predictions, fold_targets, fold_dates = [], [], []\n",
    "\n",
    "    # === build fold timeline ======================================\n",
    "    start_date, end_date = dates[0], dates[-1]\n",
    "    fold_start_dates, cur = [], start_date + pd.DateOffset(years=train_window_years)\n",
    "    while cur + pd.DateOffset(years=test_window_years) <= end_date:\n",
    "        fold_start_dates.append(cur)\n",
    "        cur += pd.DateOffset(years=test_window_years)\n",
    "    if do_print:\n",
    "        print(f\"Starting {window_strategy} window training with {len(fold_start_dates)} folds\")\n",
    "    seq_models = ['lstm', 'transformer', 'tcn', 'temporalconvnet',\n",
    "                'cnn', 'conv1d', 'simpleconvolutional','temporalfusiontransformer']\n",
    "\n",
    "    # === walk through every fold ==================================\n",
    "    for fold_idx, test_start_date in enumerate(fold_start_dates, 1):\n",
    "        if do_print:\n",
    "            print(f\"\\n— Fold {fold_idx}/{len(fold_start_dates)} —\")\n",
    "        test_end_date  = test_start_date + pd.DateOffset(years=test_window_years)\n",
    "        train_start_date = (test_start_date - pd.DateOffset(years=train_window_years)\n",
    "                            if window_strategy == 'rolling' else start_date)\n",
    "\n",
    "        train_mask = (dates >= train_start_date) & (dates < test_start_date)\n",
    "        test_mask  = (dates >= test_start_date) & (dates < test_end_date)\n",
    "        if train_mask.sum() == 0 or test_mask.sum() == 0:\n",
    "            print(\"Skipping fold – insufficient data\")\n",
    "            continue\n",
    "\n",
    "        # ----------- slice data ----------------------------------\n",
    "        X_train, y_train = X[train_mask], y[train_mask]\n",
    "        X_test,  y_test  = X[test_mask],  y[test_mask]\n",
    "        train_dates, test_dates = dates[train_mask], dates[test_mask]\n",
    "\n",
    "        # ----------- scaling -------------------------------------\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "        # ----------- optional autoencoder ------------------------\n",
    "        autoencoder = None\n",
    "        if use_autoencoder:\n",
    "            autoencoder = train_autoencoder(X_train_scaled, encoding_dim, epochs=50, lr=lr, device=device)\n",
    "            with torch.no_grad():\n",
    "                X_train_scaled = autoencoder.encode(torch.FloatTensor(X_train_scaled).to(device)).cpu().numpy()\n",
    "                X_test_scaled  = autoencoder.encode(torch.FloatTensor(X_test_scaled) .to(device)).cpu().numpy()\n",
    "\n",
    "        # ----------- AR‑lag augmentation -------------------------\n",
    "        X_train_tensor, y_train_tensor = prepare_data_for_model(X_train_scaled, y_train,\n",
    "                                                                model_type, seq_length, ar_lags)\n",
    "        X_test_tensor,  y_test_tensor  = prepare_data_for_model(X_test_scaled,  y_test,\n",
    "                                                                model_type, seq_length, ar_lags)\n",
    "        offset = max(seq_length, ar_lags) if model_type.lower() in seq_models else ar_lags\n",
    "        train_dates_adj, test_dates_adj = train_dates[offset:], test_dates[offset:]\n",
    "\n",
    "        # ----------- model / optimiser ---------------------------\n",
    "        input_dim = X_train_tensor.shape[-1]\n",
    "        model     = model_class(input_dim=input_dim, **model_kwargs).to(device)\n",
    "        criterion = ElasticNetLoss(model=model, alpha=alpha, l1_ratio=l1_ratio)\n",
    "    #         criterion = lambda predictions, targets: (\n",
    "    #     SharpeRatioLoss(risk_free_rate=np.mean(tbill3m[train_mask]))(predictions, targets)\n",
    "    #     + 5*nn.MSELoss()(predictions, targets)\n",
    "    # )\n",
    "        \n",
    "        optimizer = get_optimizer_for_model(model, model_type, lr)\n",
    "        scheduler = get_learning_rate_scheduler(optimizer, model_type)\n",
    "\n",
    "        train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "        test_loader  = DataLoader(TensorDataset(X_test_tensor,  y_test_tensor),  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # ----------- training loop -------------------------------\n",
    "        if do_print:\n",
    "            print(\"Training final model…\")\n",
    "            final_epochs = epochs // 2 if walk_forward_cv else epochs\n",
    "            for ep in range(final_epochs):\n",
    "                loss = train_model_epoch(model, train_loader, criterion, optimizer, scheduler=scheduler,\n",
    "                                        model_type=model_type, device=device)\n",
    "                if (ep + 1) % 20 == 0:\n",
    "                    print(f\"  epoch {ep+1}/{final_epochs}  loss={loss:.6f}\")\n",
    "\n",
    "        # ----------- evaluation ----------------------------------\n",
    "        _, train_predictions, train_targets = evaluate_model(model, train_loader, criterion, device)\n",
    "        _, test_predictions,  test_targets  = evaluate_model(model, test_loader,  criterion, device)\n",
    "\n",
    "        # ----------- metric calc ---------------------------------\n",
    "        train_mse = mean_squared_error(train_targets, train_predictions)\n",
    "        test_mse  = mean_squared_error(test_targets,  test_predictions)\n",
    "        train_mae = mean_absolute_error(train_targets, train_predictions)\n",
    "        test_mae  = mean_absolute_error(test_targets,  test_predictions)\n",
    "        train_r2  = r2_score(train_targets, train_predictions)\n",
    "        test_r2   = r2_score(test_targets,  test_predictions)\n",
    "\n",
    "        train_positions = np.tanh(train_predictions)\n",
    "        test_positions  = np.tanh(test_predictions)\n",
    "\n",
    "        train_strategy_returns = train_positions * train_targets\n",
    "        test_strategy_returns  = test_positions  * test_targets\n",
    "\n",
    "        train_rf = np.mean(tbill3m[train_mask])\n",
    "        test_rf  = np.mean(tbill3m[test_mask])\n",
    "\n",
    "        train_sharpe  = calculate_sharpe_ratio(train_strategy_returns, train_rf)\n",
    "        test_sharpe   = calculate_sharpe_ratio(test_strategy_returns,  test_rf)\n",
    "        train_sortino = calculate_sortino_ratio(train_strategy_returns, train_rf)\n",
    "        test_sortino  = calculate_sortino_ratio(test_strategy_returns,  test_rf)\n",
    "\n",
    "        train_hit = hit_rate(train_predictions, train_targets)\n",
    "        test_hit  = hit_rate(test_predictions,  test_targets)\n",
    "        if do_print:\n",
    "            print(f\"Train MSE {train_mse:.6f} | MAE {train_mae:.6f} | R² {train_r2:.4f} | Sharpe {train_sharpe:.3f} | Sortino {train_sortino:.3f} | Hit {train_hit:.2%}\")\n",
    "            print(f\"Test  MSE {test_mse:.6f} | MAE {test_mae:.6f} | R² {test_r2:.4f} | Sharpe {test_sharpe:.3f} | Sortino {test_sortino:.3f} | Hit {test_hit:.2%}\")\n",
    "\n",
    "        # Store results - everything below remains the same\n",
    "        fold_result = {\n",
    "            'fold': fold_idx,\n",
    "            'train_period': (train_dates[0], train_dates[-1]),\n",
    "            'test_period': (test_dates[0], test_dates[-1]),\n",
    "            'train_mse': train_mse,\n",
    "            'test_mse': test_mse,\n",
    "            'train_sharpe': train_sharpe,\n",
    "            'test_sharpe': test_sharpe,\n",
    "            'train_mae': train_mae,\n",
    "            'test_mae': test_mae,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'train_sortino': train_sortino,\n",
    "            'test_sortino': test_sortino,\n",
    "            'train_hit': train_hit,\n",
    "            'test_hit': test_hit,\n",
    "            'train_predictions': train_predictions,\n",
    "            'test_predictions': test_predictions,\n",
    "            'train_targets': train_targets,\n",
    "            'test_targets': test_targets,\n",
    "            'train_dates': train_dates_adj,\n",
    "            'test_dates': test_dates_adj\n",
    "        }\n",
    "        \n",
    "        results['fold_results'].append(fold_result)\n",
    "        results['scalers'].append(scaler)\n",
    "        results['autoencoders'].append(autoencoder)\n",
    "        results['models'].append(model.state_dict())\n",
    "        \n",
    "        # Accumulate for overall plotting\n",
    "        results['all_train_predictions'].extend(train_predictions)\n",
    "        results['all_train_targets'].extend(train_targets)\n",
    "        results['all_test_predictions'].extend(test_predictions)\n",
    "        results['all_test_targets'].extend(test_targets)\n",
    "        results['all_train_dates'].extend(train_dates_adj)\n",
    "        results['all_test_dates'].extend(test_dates_adj)\n",
    "        \n",
    "        results['metrics']['train_mse'].append(train_mse)\n",
    "        results['metrics']['test_mse'].append(test_mse)\n",
    "        results['metrics']['train_sharpe'].append(train_sharpe)\n",
    "        results['metrics']['test_sharpe'].append(test_sharpe)\n",
    "        results['metrics']['train_mae'].append(train_mae)\n",
    "        results['metrics']['test_mae'].append(test_mae)\n",
    "        results['metrics']['train_r2'].append(train_r2)\n",
    "        results['metrics']['test_r2'].append(test_r2)\n",
    "        results['metrics']['train_sortino'].append(train_sortino)\n",
    "        results['metrics']['test_sortino'].append(test_sortino)\n",
    "        results['metrics']['train_hit'].append(train_hit)\n",
    "        results['metrics']['test_hit'].append(test_hit)\n",
    "        \n",
    "        # Store for individual fold plotting\n",
    "        fold_predictions.append((train_predictions, test_predictions))\n",
    "        fold_targets.append((train_targets, test_targets))\n",
    "        fold_dates.append((train_dates_adj, test_dates_adj))\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    if results['metrics']['train_mse']:\n",
    "        results['overall_metrics'] = {\n",
    "            'avg_train_mse': np.mean(results['metrics']['train_mse']),\n",
    "            'avg_test_mse': np.mean(results['metrics']['test_mse']),\n",
    "            'avg_train_sharpe': np.mean(results['metrics']['train_sharpe']),\n",
    "            'avg_test_sharpe': np.mean(results['metrics']['test_sharpe']),\n",
    "            'std_train_mse': np.std(results['metrics']['train_mse']),\n",
    "            'std_test_mse': np.std(results['metrics']['test_mse']),\n",
    "            'std_train_sharpe': np.std(results['metrics']['train_sharpe']),\n",
    "            'std_test_sharpe': np.std(results['metrics']['test_sharpe']),\n",
    "            'avg_train_mae': np.mean(results['metrics']['train_mae']),\n",
    "            'avg_test_mae': np.mean(results['metrics']['test_mae']),\n",
    "            'std_train_mae': np.std(results['metrics']['train_mae']),\n",
    "            'std_test_mae': np.std(results['metrics']['test_mae']),\n",
    "            'avg_train_r2': np.mean(results['metrics']['train_r2']),\n",
    "            'avg_test_r2': np.mean(results['metrics']['test_r2']),\n",
    "            'std_train_r2': np.std(results['metrics']['train_r2']),\n",
    "            'std_test_r2': np.std(results['metrics']['test_r2']),\n",
    "            'avg_train_sortino': np.mean(results['metrics']['train_sortino']),\n",
    "            'avg_test_sortino': np.mean(results['metrics']['test_sortino']),\n",
    "            'std_train_sortino': np.std(results['metrics']['train_sortino']),\n",
    "            'std_test_sortino': np.std(results['metrics']['test_sortino']),\n",
    "            'avg_train_hit': np.mean(results['metrics']['train_hit']),\n",
    "            'avg_test_hit': np.mean(results['metrics']['test_hit']),\n",
    "            'std_train_hit': np.std(results['metrics']['train_hit']),\n",
    "            'std_test_hit': np.std(results['metrics']['test_hit'])\n",
    "        }\n",
    "        if do_print:\n",
    "            print(\"\\n=== OVERALL RESULTS ===\")\n",
    "            print(f\"Average Train MSE: {results['overall_metrics']['avg_train_mse']:.6f} ± {results['overall_metrics']['std_train_mse']:.6f}\")\n",
    "            print(f\"Average Test MSE: {results['overall_metrics']['avg_test_mse']:.6f} ± {results['overall_metrics']['std_test_mse']:.6f}\")\n",
    "            print(f\"Average Train Sharpe: {results['overall_metrics']['avg_train_sharpe']:.4f} ± {results['overall_metrics']['std_train_sharpe']:.4f}\")\n",
    "            print(f\"Average Test Sharpe: {results['overall_metrics']['avg_test_sharpe']:.4f} ± {results['overall_metrics']['std_test_sharpe']:.4f}\")\n",
    "            print(f\"Average Train MAE: {results['overall_metrics']['avg_train_mae']:.6f} ± {results['overall_metrics']['std_train_mae']:.6f}\")\n",
    "            print(f\"Average Test MAE: {results['overall_metrics']['avg_test_mae']:.6f} ± {results['overall_metrics']['std_test_mae']:.6f}\")\n",
    "            print(f\"Average Train R²: {results['overall_metrics']['avg_train_r2']:.4f} ± {results['overall_metrics']['std_train_r2']:.4f}\")\n",
    "            print(f\"Average Test R²: {results['overall_metrics']['avg_test_r2']:.4f} ± {results['overall_metrics']['std_test_r2']:.4f}\")\n",
    "            print(f\"Average Train Sortino: {results['overall_metrics']['avg_train_sortino']:.4f} ± {results['overall_metrics']['std_train_sortino']:.4f}\") \n",
    "            print(f\"Average Test Sortino: {results['overall_metrics']['avg_test_sortino']:.4f} ± {results['overall_metrics']['std_test_sortino']:.4f}\")\n",
    "            print(f\"Average Train Hit Rate: {results['overall_metrics']['avg_train_hit']:.2%} ± {results['overall_metrics']['std_train_hit']:.2%}\")\n",
    "            print(f\"Average Test Hit Rate: {results['overall_metrics']['avg_test_hit']:.2%} ± {results['overall_metrics']['std_test_hit']:.2%}\")\n",
    "\n",
    "    \n",
    "    # Plotting\n",
    "    if plot_results and fold_predictions:\n",
    "        n_plots = len(fold_predictions)\n",
    "        fig, axes = plt.subplots(n_plots, 1, figsize=(15, 4 * n_plots))\n",
    "        if n_plots == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, (fold_pred, fold_targ, fold_date) in enumerate(zip(fold_predictions, fold_targets, fold_dates)):\n",
    "            train_pred, test_pred = fold_pred\n",
    "            train_targ, test_targ = fold_targ\n",
    "            train_date, test_date = fold_date\n",
    "            \n",
    "            ax = axes[i]\n",
    "\n",
    "            # Plot actual returns (always black)\n",
    "            ax.plot(train_date, train_targ, 'k-', label='Actual Returns', linewidth=1)\n",
    "            ax.plot(test_date, test_targ, 'k-', linewidth=1)\n",
    "            \n",
    "            # Plot predictions (blue for train, red for test)\n",
    "            ax.plot(train_date, train_pred, 'b-', label='Train Predictions', alpha=0.7)\n",
    "            ax.plot(test_date, test_pred, 'r-', label='Test Predictions', alpha=0.7)\n",
    "            \n",
    "            # Add vertical line to separate train/test\n",
    "            if len(test_date) > 0:\n",
    "                ax.axvline(x=test_date[0], color='gray', linestyle='--', alpha=0.5)\n",
    "            \n",
    "            ax.set_title(f'Fold {i+1}: Predictions vs Actual Returns')\n",
    "            ax.set_ylabel('Returns')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Overall performance plot\n",
    "        if len(results['all_train_dates']) > 0:\n",
    "            plt.figure(figsize=(15, 6))\n",
    "            \n",
    "            # Combine and sort all data by date\n",
    "            all_data = []\n",
    "            for train_pred, train_targ, train_date in zip(\n",
    "                [results['fold_results'][i]['train_predictions'] for i in range(len(results['fold_results']))],\n",
    "                [results['fold_results'][i]['train_targets'] for i in range(len(results['fold_results']))],\n",
    "                [results['fold_results'][i]['train_dates'] for i in range(len(results['fold_results']))]\n",
    "            ):\n",
    "                for pred, targ, date in zip(train_pred, train_targ, train_date):\n",
    "                    all_data.append((date, targ, pred, 'train'))\n",
    "            \n",
    "            for test_pred, test_targ, test_date in zip(\n",
    "                [results['fold_results'][i]['test_predictions'] for i in range(len(results['fold_results']))],\n",
    "                [results['fold_results'][i]['test_targets'] for i in range(len(results['fold_results']))],\n",
    "                [results['fold_results'][i]['test_dates'] for i in range(len(results['fold_results']))]\n",
    "            ):\n",
    "                for pred, targ, date in zip(test_pred, test_targ, test_date):\n",
    "                    all_data.append((date, targ, pred, 'test'))\n",
    "            \n",
    "            # Sort by date\n",
    "            all_data.sort(key=lambda x: x[0])\n",
    "            \n",
    "            # Separate data\n",
    "            dates_all = [x[0] for x in all_data]\n",
    "            targets_all = [x[1] for x in all_data]\n",
    "            predictions_all = [x[2] for x in all_data]\n",
    "            types_all = [x[3] for x in all_data]\n",
    "            \n",
    "            # Plot\n",
    "            plt.plot(dates_all, targets_all, 'k-', label='Actual Returns', linewidth=1)\n",
    "            \n",
    "            # Plot predictions with different colors\n",
    "            train_mask = np.array(types_all) == 'train'\n",
    "            test_mask = np.array(types_all) == 'test'\n",
    "            \n",
    "            if np.any(train_mask):\n",
    "                plt.plot(np.array(dates_all)[train_mask], np.array(predictions_all)[train_mask], \n",
    "                        'b.', label='Train Predictions', alpha=0.7, markersize=3)\n",
    "            if np.any(test_mask):\n",
    "                plt.plot(np.array(dates_all)[test_mask], np.array(predictions_all)[test_mask], \n",
    "                        'r.', label='Test Predictions', alpha=0.7, markersize=3)\n",
    "            \n",
    "            plt.title('Overall Model Performance: Predictions vs Actual Returns')\n",
    "            plt.ylabel('Returns')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a95400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:15:07.884369Z",
     "start_time": "2025-06-18T20:15:07.847320Z"
    }
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"data_non_std.csv\",parse_dates=[\"Unnamed: 0\"]).rename(columns={'Unnamed: 0': 'Date'}) \n",
    "features = df.drop(columns=[\"returns\", \"Date\"])\n",
    "target = df[\"returns\"].values.astype(np.float32)\n",
    "dates = pd.to_datetime(df[\"Date\"]).astype('datetime64[ns]').tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61acb33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T20:29:34.991988Z",
     "start_time": "2025-06-18T20:15:10.048397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting rolling window training with 13 folds\n",
      "\n",
      "— Fold 1/13 —\n",
      "Training final model…\n"
     ]
    }
   ],
   "source": [
    "class SimpleFeedForward(nn.Module):\n",
    "    \"\"\"Example feedforward model\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=200, dropout=0):\n",
    "        super(SimpleFeedForward, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"Example LSTM model\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=200, num_layers=2, dropout=0):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(dropout),\n",
    "                                nn.Linear(hidden_dim // 2, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])  # Use last time step output\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 model_dim: int = 128,   # divisible by nhead\n",
    "                 nhead: int = 8,\n",
    "                 num_layers: int = 2,\n",
    "                 dropout: float = 0.1,\n",
    "                 max_seq_length: int = 500):  # for positional encoding\n",
    "        super().__init__()\n",
    "\n",
    "        # project raw features to model_dim\n",
    "        self.input_proj = nn.Linear(input_dim, model_dim)\n",
    "        \n",
    "        # Add positional encoding \n",
    "        self.pos_encoding = PositionalEncoding(model_dim, dropout, max_seq_length)\n",
    "\n",
    "        # vanilla encoder stack\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=model_dim,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=model_dim * 4,  # standard practice\n",
    "                dropout=dropout,\n",
    "                activation='gelu',  # often works better than relu for transformers\n",
    "                batch_first=True        # so x is (B, T, F)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # regression head with additional processing\n",
    "        self.norm = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(model_dim, 1)\n",
    "\n",
    "    def forward(self, x):               # x: (B, T, input_dim)\n",
    "        x = self.input_proj(x)          # (B, T, model_dim)\n",
    "        x = self.pos_encoding(x)        # Add positional information\n",
    "        x = self.encoder(x)             # (B, T, model_dim)\n",
    "        x = self.norm(x[:, -1])         # Layer norm on last time step\n",
    "        x = self.dropout(x)             # Additional dropout\n",
    "        return self.fc(x)               # (B, 1)\n",
    "\n",
    "class SimpleConvolutional(nn.Module):\n",
    "    \"\"\"\n",
    "    1-D CNN for sequence-to-one forecasting.\n",
    "    Assumes input tensor shape: (batch, seq_len, n_features)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,               # -- n_features after any AR-lag concat\n",
    "        num_channels: List[int] = [32, 64, 32],\n",
    "        kernel_size: int = 5,\n",
    "        dropout: float = 0.25,\n",
    "        seq_length: int = 12          # 〈NEW – keep default same as pipeline〉\n",
    "    ):\n",
    "        super(SimpleConvolutional, self).__init__()\n",
    "\n",
    "        layers, in_channels = [], input_dim\n",
    "        for out_channels in num_channels:\n",
    "            layers += [\n",
    "                nn.Conv1d(in_channels, out_channels,\n",
    "                          kernel_size, padding=kernel_size // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(2),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "        # ── force length-1 feature map so fc dim is invariant to seq_length ──\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)                   # ← NEW\n",
    "        self.fc   = nn.Linear(in_channels, 1)                 # 〈in_channels == last out_channels〉\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, n_features)\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, 2)          # (batch, n_features, seq_len)\n",
    "        x = self.conv(x)               # (batch, channels, L)\n",
    "        x = self.pool(x)               # (batch, channels, 1)\n",
    "        x = x.view(x.size(0), -1)      # (batch, channels)\n",
    "        return self.fc(x).squeeze(-1)  # (batch,)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.transpose(0, 1)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "tbill3m_data = df[\"tbill3m\"].values.astype(np.float32)\n",
    "\n",
    "results_tcn = sp500_training_pipeline(\n",
    "    X=features,\n",
    "    y=target,\n",
    "    dates=dates,\n",
    "    tbill3m=tbill3m_data,\n",
    "   model_class  = TemporalConvNet,\n",
    "    model_type   = 'temporalconvnet',                    # or 'simpleconvolutional'\n",
    "    model_kwargs = {\n",
    "        'num_channels': [32, 64,128,64, 32], 'kernel_size': 5, 'dropout': 0.1\n",
    "    },\n",
    "    window_strategy='rolling',\n",
    "    train_window_years=3,\n",
    "    test_window_years=1,\n",
    "    use_autoencoder=True,\n",
    "    encoding_dim=10,\n",
    "    seq_length=24,\n",
    "    ar_lags=1,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    lr=0.001,\n",
    "    plot_results=True,\n",
    "    alpha=0.0,\n",
    "    l1_ratio=0.0,\n",
    ")\n",
    "\n",
    "\"\"\"model_kwargs={\n",
    "        'num_channels':[32, 64, 32], 'kernel_size':5, 'dropout':0.25\n",
    "    }\"\"\"\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (volatilityenv)",
   "language": "python",
   "name": "volatilityenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
